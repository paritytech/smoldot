// Smoldot
// Copyright (C) 2019-2022  Parity Technologies (UK) Ltd.
// SPDX-License-Identifier: GPL-3.0-or-later WITH Classpath-exception-2.0

// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.

// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.

// You should have received a copy of the GNU General Public License
// along with this program.  If not, see <http://www.gnu.org/licenses/>.

//! Collection of libp2p connections.
//!
//! The [`Network`] struct in this module is a collection of libp2p connections. In the
//! documentation below, it is also called the *coordinator*.
//!
//! When a connection is inserted in the collection with [`Network::insert`], two objects are
//! returned: an identifier for this new connection assigned by the collection, and a
//! [`ConnectionTask`].
//!
//! All the [`ConnectionTask`]s created by the [`Network`] communicate with that [`Network`] by
//! passing messages. Passing the messages has to be done explicitly by the API user. It is the
//! responsibility of the API user to pull messages from the coordinator (i.e. the [`Network`])
//! and push them onto the [`ConnectionTask`] and vice-versa.
//!
//! # Usage
//!
//! - Whenever a new connection is established, use [`Network::insert`] to allocate a connection
//! in the collection.
//! - When a connection has received data or is ready to send more data, use
//! [`ConnectionTask::read_write`] to synchronize the state of the [`ConnectionTask`] with the
//! actual state of the connection.
//! - In parallel, continuously call [`Network::next_event`] to process the events generated by
//! the calls to [`Network::read_write`].
//!

use super::connection::{established, handshake, NoiseKey};
use alloc::{
    collections::{BTreeMap, BTreeSet, VecDeque},
    string::{String, ToString as _},
    sync::Arc,
    vec::Vec,
};
use core::{
    iter, mem,
    ops::{self, Add, Sub},
    time::Duration,
};
use rand::Rng as _;
use rand_chacha::{rand_core::SeedableRng as _, ChaCha20Rng};

pub use super::peer_id::PeerId;
pub use super::read_write::ReadWrite;
pub use established::{
    ConfigRequestResponse, ConfigRequestResponseIn, InboundError, NotificationsOutErr,
};

/// Configuration for a [`Network`].
pub struct Config {
    /// Seed for the randomness within the networking state machine.
    pub randomness_seed: [u8; 32],

    /// Number of connections containers should initially allocate for.
    pub capacity: usize,

    pub notification_protocols: Vec<NotificationProtocolConfig>,

    pub request_response_protocols: Vec<ConfigRequestResponse>,

    /// Amount of time after which a connection handshake is considered to have taken too long
    /// and must be aborted.
    pub handshake_timeout: Duration,

    /// Name of the ping protocol on the network.
    pub ping_protocol: String,

    /// Key used for the encryption layer.
    /// This is a Noise static key, according to the Noise specification.
    /// Signed using the actual libp2p key.
    pub noise_key: NoiseKey,
}

/// Configuration for a specific overlay network.
///
/// See [`Config::notification_protocols`].
pub struct NotificationProtocolConfig {
    /// Name of the protocol negotiated on the wire.
    pub protocol_name: String,

    /// Optional alternative names for this protocol. Can represent different versions.
    ///
    /// Negotiated in order in which they are passed.
    // TODO: presently unused
    pub fallback_protocol_names: Vec<String>,

    /// Maximum size, in bytes, of the handshake that can be received.
    pub max_handshake_size: usize,

    /// Maximum size, in bytes, of a notification that can be received.
    pub max_notification_size: usize,
}

/// Identifier of a connection spawned by the [`Network`].
//
// Identifiers are never reused.
#[derive(Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]
pub struct ConnectionId(u64);

impl ConnectionId {
    pub fn min_value() -> Self {
        ConnectionId(u64::min_value())
    }

    pub fn max_value() -> Self {
        ConnectionId(u64::max_value())
    }
}

/// Identifier of a request created by [`Network::start_request`].
//
// Identifiers are never reused.
// TODO: update doc ^
#[derive(Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]
pub struct SubstreamId(u64);

impl SubstreamId {
    pub fn min_value() -> Self {
        SubstreamId(u64::min_value())
    }

    pub fn max_value() -> Self {
        SubstreamId(u64::max_value())
    }
}

/// Data structure containing the list of all connections, pending or not, and their latest known
/// state. See also [the module-level documentation](..).
pub struct Network<TConn, TNow> {
    /// Messages waiting to be sent to connection tasks.
    messages_to_connections: VecDeque<(ConnectionId, CoordinatorToConnectionInner<TNow>)>,

    /// Messages received from connection tasks. Processed in [`Network::next_event`].
    pending_incoming_messages: VecDeque<(ConnectionId, ConnectionToCoordinatorInner)>,

    /// Id to assign to the next connection.
    next_connection_id: ConnectionId,

    /// Id to assign to the next substream, such as the next request or next notifications
    /// substream.
    ///
    /// All substreams share the same pool of ids across all connections.
    next_substream_id: SubstreamId,

    /// List of all connections in the data structure.
    connections: hashbrown::HashMap<ConnectionId, Connection<TConn>, fnv::FnvBuildHasher>,

    /// If `Some`, the given connection is in the process of shutting down. Calling
    /// [`Network::next_event`] will cancel all ongoing requests and notification substreams
    /// that concern this connection before processing any other incoming message.
    shutting_down_connection: Option<ConnectionId>,

    // TODO: review; maybe it's possible to remove data
    outgoing_notification_substreams:
        hashbrown::HashMap<SubstreamId, (ConnectionId, OutSubstreamState), fnv::FnvBuildHasher>,

    // TODO: review; maybe it's possible to remove data
    outgoing_notification_substreams_by_connection: BTreeSet<(ConnectionId, SubstreamId)>,

    // TODO: review; maybe it's possible to remove data
    outgoing_requests: hashbrown::HashMap<SubstreamId, ConnectionId, fnv::FnvBuildHasher>,

    /// Always contains the same entries as [`Network::outgoing_requests`] but ordered differently.
    // TODO: review; maybe it's possible to remove data
    outgoing_requests_by_connection: BTreeSet<(ConnectionId, SubstreamId)>,

    // TODO: review; maybe it's possible to remove data
    ingoing_notification_substreams: hashbrown::HashMap<
        SubstreamId,
        (ConnectionId, InSubstreamState, established::SubstreamId),
        fnv::FnvBuildHasher,
    >,

    // TODO: review; maybe it's possible to remove data
    ingoing_notification_substreams_by_connection:
        BTreeMap<(ConnectionId, established::SubstreamId), SubstreamId>,

    /// List of requests that connections have received and haven't been answered by the API user
    /// yet.
    // TODO: review; maybe it's possible to remove data
    ingoing_requests: hashbrown::HashMap<
        SubstreamId,
        (ConnectionId, established::SubstreamId),
        fnv::FnvBuildHasher,
    >,

    /// Always contains the same entries as [`Network::ingoing_requests`] but ordered differently.
    // TODO: review; maybe it's possible to remove data
    ingoing_requests_by_connection: BTreeSet<(ConnectionId, SubstreamId)>,

    /// Generator for randomness seeds given to the established connections.
    randomness_seeds: ChaCha20Rng,

    /// See [`Config::handshake_timeout`].
    handshake_timeout: Duration,

    /// See [`Config::noise_key`].
    noise_key: Arc<NoiseKey>,

    /// See [`OverlayNetwork`].
    notification_protocols: Arc<[OverlayNetwork]>,

    /// See [`Config::request_response_protocols`].
    request_response_protocols: Arc<[ConfigRequestResponse]>,

    /// See [`Config::ping_protocol`].
    ping_protocol: Arc<str>,
}

struct Connection<TConn> {
    /// `true` if the connection is still in its handshake phase.
    handshaking: bool,

    /// `true` if [`Network::start_shutdown`] has been called on this connection.
    ///
    /// Even if the remote starts the shutdown at the same time, from an API perspective if this
    /// flag is `true` it will be considered as if the API user had initiated the shutdown.
    start_shutdown_called: bool,

    /// `true` if either [`Network::start_shutdown`] has been called on this connection, or a
    /// [`CoordinatorToConnectionInner::StartShutdown`] has been received.
    shutting_down: bool,

    user_data: TConn,
}

/// State of a specific overlay network.
///
/// This struct is a slight variation to [`NotificationProtocolConfig`].
struct OverlayNetwork {
    /// See [`NotificationProtocolConfig`].
    config: NotificationProtocolConfig,
}

/// See [`Network::outgoing_notification_substreams`].
///
/// > **Note**: There is no `Closed` variant, as this corresponds to a lack of entry in the map.
#[derive(Debug, Copy, Clone, PartialEq, Eq, Ord, PartialOrd, Hash)]
enum OutSubstreamState {
    /// Substream hasn't been accepted or refused yet.
    Pending,
    Open,
}

/// See [`Network::ingoing_notification_substreams`].
///
/// > **Note**: There is no `Closed` variant, as this corresponds to a lack of entry in the map.
// TODO: merge with out state
#[derive(Debug, Copy, Clone, PartialEq, Eq, Ord, PartialOrd, Hash)]
enum InSubstreamState {
    /// Substream hasn't been accepted or refused yet. Contains the overlay network index.
    Pending,
    Open,
}

impl<TConn, TNow> Network<TConn, TNow>
where
    TNow: Clone + Add<Duration, Output = TNow> + Sub<TNow, Output = Duration> + Ord,
{
    /// Initializes a new network data structure.
    pub fn new(config: Config) -> Self {
        let notification_protocols = config
            .notification_protocols
            .into_iter()
            .map(|config| OverlayNetwork { config })
            .collect::<Arc<[_]>>();

        Network {
            messages_to_connections: VecDeque::new(), // TODO: capacity?
            pending_incoming_messages: VecDeque::new(), // TODO: capacity?
            next_substream_id: SubstreamId(0),
            handshake_timeout: config.handshake_timeout,
            next_connection_id: ConnectionId(0),
            connections: hashbrown::HashMap::with_capacity_and_hasher(
                config.capacity,
                Default::default(),
            ),
            shutting_down_connection: None,
            outgoing_requests: hashbrown::HashMap::with_capacity_and_hasher(0, Default::default()), // TODO: capacity?,
            outgoing_requests_by_connection: BTreeSet::new(),
            ingoing_requests: hashbrown::HashMap::with_capacity_and_hasher(0, Default::default()), // TODO: capacity?
            ingoing_requests_by_connection: BTreeSet::new(),
            outgoing_notification_substreams: hashbrown::HashMap::with_capacity_and_hasher(
                0,
                Default::default(),
            ), // TODO: capacity?
            outgoing_notification_substreams_by_connection: BTreeSet::new(),
            ingoing_notification_substreams: hashbrown::HashMap::with_capacity_and_hasher(
                0,
                Default::default(),
            ), // TODO: capacity?
            ingoing_notification_substreams_by_connection: BTreeMap::new(),
            randomness_seeds: ChaCha20Rng::from_seed(config.randomness_seed),
            noise_key: Arc::new(config.noise_key),
            notification_protocols,
            request_response_protocols: config.request_response_protocols.into_iter().collect(), // TODO: stupid overhead
            ping_protocol: config.ping_protocol.into(),
        }
    }

    /// Adds a new connection to the collection.
    ///
    /// Must be passed the moment (as a `TNow`) when the connection as been established, in order
    /// to determine when the handshake timeout expires.
    ///
    /// `is_initiator` must be `true` if the connection has been initiated locally, or `false` if
    /// it has been initiated by the remote.
    pub fn insert(
        &mut self,
        when_connected: TNow,
        is_initiator: bool,
        user_data: TConn,
    ) -> (ConnectionId, ConnectionTask<TNow>) {
        let connection_id = self.next_connection_id;
        self.next_connection_id.0 += 1;

        let connection_task = ConnectionTask {
            connection: ConnectionInner::Handshake {
                handshake: handshake::HealthyHandshake::new(is_initiator),
                randomness_seed: self.randomness_seeds.gen(),
                timeout: when_connected + self.handshake_timeout,
                noise_key: self.noise_key.clone(),
                notification_protocols: self.notification_protocols.clone(),
                request_response_protocols: self.request_response_protocols.clone(),
                ping_protocol: self.ping_protocol.clone(),
            },
            pending_messages: VecDeque::with_capacity({
                // We never buffer more than a few messages.
                4
            }),
        };

        let _previous_value = self.connections.insert(
            connection_id,
            Connection {
                handshaking: true,
                shutting_down: false,
                start_shutdown_called: false,
                user_data,
            },
        );
        debug_assert!(_previous_value.is_none());

        (connection_id, connection_task)
    }

    /// Switches the connection to a state where it will shut down soon.
    ///
    /// Calling this function does **not** generate a [`Event::StartShutdown`] event for this
    /// connection. The event is implied.
    ///
    /// It is no longer possible to start requests, open notifications substreams, or send
    /// notifications. No new incoming reuqests or notification substreams will be reported. The
    /// incoming notifications that were sent by the remote before the shutdown started will still
    /// be reported.
    ///
    /// It is possible to call this method on connection that are still in their handshaking
    /// phase.
    ///
    /// This function generates a message destined to the connection. Use
    /// [`Network::pull_message_to_connection`] to process these messages after it has returned.
    ///
    /// # Panic
    ///
    /// Panics if the connection is already shutting down, either because
    /// [`Network::start_shutdown`] was called or a [`Event::StartShutdown`] event was yielded.
    ///
    pub fn start_shutdown(&mut self, connection_id: ConnectionId) {
        let connection = self.connections.get_mut(&connection_id).unwrap();
        assert!(!connection.start_shutdown_called);
        assert!(!connection.shutting_down);
        connection.start_shutdown_called = true;
        connection.shutting_down = true;

        self.messages_to_connections
            .push_back((connection_id, CoordinatorToConnectionInner::StartShutdown));
    }

    /// Returns the number of connections in the collection.
    pub fn len(&self) -> usize {
        self.connections.len()
    }

    /// Returns the Noise key originalled passed as [`Config::noise_key`].
    pub fn noise_key(&self) -> &NoiseKey {
        &self.noise_key
    }

    /// Returns the list the overlay networks originally passed as
    /// [`Config::notification_protocols`].
    pub fn notification_protocols(
        &self,
    ) -> impl ExactSizeIterator<Item = &NotificationProtocolConfig> {
        self.notification_protocols.iter().map(|v| &v.config)
    }

    /// Returns the list the request-response protocols originally passed as
    /// [`Config::request_response_protocols`].
    pub fn request_response_protocols(
        &self,
    ) -> impl ExactSizeIterator<Item = &ConfigRequestResponse> {
        self.request_response_protocols.iter()
    }

    /// Sends a request to the given peer.
    ///
    /// A [`Event::Response`] event will later be generated containing the result of the request.
    /// This event is generated even if the connection the request was sent on has been closed.
    ///
    /// It is invalid to start a request on a connection before a [`Event::HandshakeFinished`]
    /// or after a [`Event::StartShutdown`] has been generated, or after
    /// [`Network::start_shutdown`] has been called.
    ///
    /// Returns a newly-allocated identifier for this substream.
    ///
    /// This function generates a message destined to the connection. Use
    /// [`Network::pull_message_to_connection`] to process these messages after it has returned.
    ///
    /// # Requests
    ///
    /// A request consists in:
    ///
    /// - Opening a substream on an established connection with the target.
    /// - Negotiating the requested protocol (`protocol_index`) on this substream using the
    ///   *multistream-select* protocol.
    /// - Sending the request (`request_data` parameter), prefixed with its length.
    /// - Waiting for the response (prefixed with its length), which is then returned.
    ///
    /// An error happens if the connection closes while the request is in progress, if the remote
    /// doesn't support the given protocol, if the request or response doesn't respect the protocol
    /// limits (see [`ConfigRequestResponse`]), or if the remote takes too much time to answer.
    ///
    /// The timeout is the time between the moment the substream is opened and the moment the
    /// response is sent back. If the emitter doesn't send the request or if the receiver doesn't
    /// answer during this time window, the request is considered failed.
    ///
    /// # Panic
    ///
    /// Panics if `protocol_index` isn't a valid index in [`Config::request_response_protocols`].
    /// Panics if the [`ConnectionId`] is invalid or is a connection that hasn't finished its
    /// handshake or is shutting down.
    ///
    pub fn start_request(
        &mut self,
        target: ConnectionId,
        protocol_index: usize,
        request_data: impl Into<Vec<u8>>,
        timeout: TNow,
    ) -> SubstreamId {
        let connection = self.connections.get(&target).unwrap();
        assert!(!connection.handshaking);
        assert!(!connection.shutting_down);

        assert!(self
            .request_response_protocols
            .get(protocol_index)
            .is_some());

        let substream_id = self.next_substream_id;
        self.next_substream_id.0 += 1;

        let _prev_value = self.outgoing_requests.insert(substream_id, target);
        debug_assert!(_prev_value.is_none());
        let _was_inserted = self
            .outgoing_requests_by_connection
            .insert((target, substream_id));
        debug_assert!(_was_inserted);

        self.messages_to_connections.push_back((
            target,
            CoordinatorToConnectionInner::StartRequest {
                protocol_index,
                request_data: request_data.into(),
                timeout,
                substream_id,
            },
        ));

        substream_id
    }

    /// Start opening a notifications substream.
    ///
    /// It is invalid to open a notifications substream on a connection before a
    /// [`Event::HandshakeFinished`] or after a [`Event::StartShutdown`] has been generated, or
    /// after [`Network::start_shutdown`] has been called.
    ///
    /// Returns a newly-allocated identifier for this substream.
    ///
    /// This function generates a message destined to the connection. Use
    /// [`Network::pull_message_to_connection`] to process these messages after it has returned.
    ///
    /// # Panic
    ///
    /// Panics if `overlay_network_index` isn't a valid index in [`Config::notification_protocols`].
    /// Panics if the [`ConnectionId`] is invalid or is a connection that hasn't finished its
    /// handshake or is shutting down.
    ///
    pub fn open_out_notifications(
        &mut self,
        connection_id: ConnectionId,
        overlay_network_index: usize,
        now: TNow,
        handshake: impl Into<Vec<u8>>,
    ) -> SubstreamId {
        let connection = self.connections.get(&connection_id).unwrap();
        assert!(!connection.handshaking);
        assert!(!connection.shutting_down);

        assert!(self
            .notification_protocols
            .get(overlay_network_index)
            .is_some());

        let substream_id = self.next_substream_id;
        self.next_substream_id.0 += 1;

        let _prev_value = self
            .outgoing_notification_substreams
            .insert(substream_id, (connection_id, OutSubstreamState::Pending));
        debug_assert!(_prev_value.is_none());
        let _was_inserted = self
            .outgoing_notification_substreams_by_connection
            .insert((connection_id, substream_id));
        debug_assert!(_was_inserted);

        self.messages_to_connections.push_back((
            connection_id,
            CoordinatorToConnectionInner::OpenOutNotifications {
                handshake: handshake.into(),
                now,
                overlay_network_index,
                substream_id,
            },
        ));

        substream_id
    }

    /// Start closing a previously-open notifications substream, or cancels opening a
    /// notifications substream.
    ///
    /// All the notifications that have been queued remain queued. The substream actually closes
    /// only once the queue is empty.
    ///
    /// Calling this method does *not* emit any event. The [`SubstreamId`] is considered invalid
    /// after this function returns.
    ///
    /// This function generates a message destined to the connection. Use
    /// [`Network::pull_message_to_connection`] to process these messages after it has returned.
    ///
    /// # Panic
    ///
    /// Panics if [`SubstreamId`] doesn't correspond to an outbound notifications substream.
    ///
    pub fn close_out_notifications(&mut self, substream_id: SubstreamId) {
        // Both `Pending` and `Open` states are accepted.
        let (connection_id, _state) = self
            .outgoing_notification_substreams
            .remove(&substream_id)
            .unwrap();
        let _was_in = self
            .outgoing_notification_substreams_by_connection
            .remove(&(connection_id, substream_id));
        debug_assert!(_was_in);

        self.messages_to_connections.push_back((
            connection_id,
            CoordinatorToConnectionInner::CloseOutNotifications { substream_id },
        ));
    }

    /// Adds a notification to the queue of notifications to send to the given peer.
    ///
    /// It is invalid to call this on a [`SubstreamId`] before a successful
    /// [`Event::NotificationsOutResult`] has been yielded.
    ///
    /// Each substream maintains a queue of notifications to be sent to the remote. This method
    /// attempts to push a notification to this queue.
    ///
    /// An error is also returned if the queue exceeds a certain size in bytes, for two reasons:
    ///
    /// - Since the content of the queue is transferred at a limited rate, each notification
    /// pushed at the end of the queue will take more time than the previous one to reach the
    /// destination. Once the queue reaches a certain size, the time it would take for
    /// newly-pushed notifications to reach the destination would start being unreasonably large.
    ///
    /// - If the remote deliberately applies back-pressure on the substream, it is undesirable to
    /// increase the memory usage of the local node.
    ///
    /// Similarly, the queue being full is a normal situation and notification protocols should
    /// be designed in such a way that discarding notifications shouldn't have a too negative
    /// impact.
    ///
    /// Regardless of the success of this function, no guarantee exists about the successful
    /// delivery of notifications.
    ///
    /// This function generates a message destined to the connection. Use
    /// [`Network::pull_message_to_connection`] to process these messages after it has returned.
    ///
    /// # Panics
    ///
    /// Panics if [`SubstreamId`] is not a fully open outbound notifications substream.
    ///
    pub fn queue_notification(
        &mut self,
        substream_id: SubstreamId,
        notification: impl Into<Vec<u8>>,
    ) -> Result<(), QueueNotificationError> {
        let (connection_id, state) = self
            .outgoing_notification_substreams
            .get(&substream_id)
            .unwrap();
        assert!(matches!(state, OutSubstreamState::Open));

        //  TODO: add some back-pressure system and return a `QueueNotificationError` if full

        self.messages_to_connections.push_back((
            *connection_id,
            CoordinatorToConnectionInner::QueueNotification {
                substream_id,
                notification: notification.into(),
            },
        ));

        Ok(())
    }

    /// Accepts a request for an inbound notifications substream reported by an
    /// [`Event::NotificationsInOpen`].
    ///
    /// If a [`Event::NotificationsInClose`] event is yielded, then this function must not be
    /// called and will panic.
    ///
    /// This function generates a message destined to the connection. Use
    /// [`Network::pull_message_to_connection`] to process these messages after it has returned.
    ///
    /// # Panic
    ///
    /// Panics if the [`SubstreamId`] doesn't correspond to an inbound notifications substream.
    ///
    pub fn accept_in_notifications(&mut self, substream_id: SubstreamId, handshake: Vec<u8>) {
        let (connection_id, state, inner_substream_id) = self
            .ingoing_notification_substreams
            .get_mut(&substream_id)
            .unwrap();
        assert!(matches!(state, InSubstreamState::Pending));

        self.messages_to_connections.push_back((
            *connection_id,
            CoordinatorToConnectionInner::AcceptInNotifications {
                substream_id: *inner_substream_id,
                handshake,
            },
        ));

        *state = InSubstreamState::Open;
    }

    /// Rejects a request for an inbound notifications substream reported by an
    /// [`Event::NotificationsInOpen`].
    ///
    /// If a [`Event::NotificationsInClose`] event is yielded, then this function must not be
    /// called and will panic.
    ///
    /// The [`SubstreamId`] is considered no longer valid after this function returns.
    ///
    /// This function generates a message destined to the connection. Use
    /// [`Network::pull_message_to_connection`] to process these messages after it has returned.
    ///
    /// # Panic
    ///
    /// Panics if the [`SubstreamId`] doesn't correspond to an inbound notifications substream.
    ///
    pub fn reject_in_notifications(&mut self, substream_id: SubstreamId) {
        if let Some((connection_id, InSubstreamState::Pending, inner_substream_id)) =
            self.ingoing_notification_substreams.remove(&substream_id)
        {
            let _was_in = self
                .ingoing_notification_substreams_by_connection
                .remove(&(connection_id, inner_substream_id));
            debug_assert_eq!(_was_in, Some(substream_id));

            self.messages_to_connections.push_back((
                connection_id,
                CoordinatorToConnectionInner::RejectInNotifications {
                    substream_id: inner_substream_id,
                },
            ));
        } else {
            // Note that, if this is reached, the pending substream is not inserted back
            // in the state machine, meaning that `self` is now in an inconsistent state.
            // But considering that we panic, this state mismatch isn't actually observable.
            panic!()
        }
    }

    /// Responds to an incoming request. Must be called in response to a [`Event::RequestIn`].
    ///
    /// If the substream was in the meanwhile yielded in an [`Event::RequestInCancel`], then this
    /// function must not be called and will panic.
    ///
    /// The [`SubstreamId`] is considered no longer valid after this function returns.
    ///
    /// This function generates a message destined to the connection. Use
    /// [`Network::pull_message_to_connection`] to process these messages after it has returned.
    ///
    /// # Panic
    ///
    /// Panics if the [`SubstreamId`] doesn't correspond to an active incoming request.
    ///
    pub fn respond_in_request(&mut self, substream_id: SubstreamId, response: Result<Vec<u8>, ()>) {
        let (connection_id, inner_substream_id) =
            self.ingoing_requests.remove(&substream_id).unwrap();
        self.ingoing_requests_by_connection
            .remove(&(connection_id, substream_id));

        self.messages_to_connections.push_back((
            connection_id,
            CoordinatorToConnectionInner::AnswerRequest {
                substream_id: inner_substream_id,
                response,
            },
        ));
    }

    /// Pulls a message that must be sent to a connection.
    ///
    /// The message must be passed to [`ConnectionTask::inject_coordinator_message`] in the
    /// appropriate connection.
    ///
    /// This function guarantees that the [`ConnectionId`] always refers to a connection that
    /// is still alive, in the sense that [`ConnectionTask::inject_coordinator_message`] has
    /// never returned `None`.
    pub fn pull_message_to_connection(
        &mut self,
    ) -> Option<(ConnectionId, CoordinatorToConnection<TNow>)> {
        self.messages_to_connections
            .pop_front()
            .map(|(id, inner)| (id, CoordinatorToConnection { inner }))
    }

    /// Injects into the state machine a message generated by
    /// [`ConnectionTask::pull_message_to_coordinator`].
    ///
    /// This message is queued and is later processed in [`Network::next_event`]. This means that
    /// it is [`Network::next_event`] and not [`Network::inject_connection_message`] that updates
    /// the internals of the state machine according to the content of the message. For example,
    /// if a [`ConnectionTask`] sends a message to the coordinator indicating that a notifications
    /// substream has been closed, the coordinator will still believe that it is open until
    /// [`Network::next_event`] processes this message and at the same time returns a corresponding
    /// [`Event`]. Processing messages directly in [`Network::inject_connection_message`] would
    /// introduce "race conditions" where the API user can't be sure in which state a connection
    /// or a substream is.
    pub fn inject_connection_message(
        &mut self,
        connection_id: ConnectionId,
        message: ConnectionToCoordinator,
    ) {
        assert!(self.connections.contains_key(&connection_id));

        // TODO: add a limit for a back-pressure-like system?
        self.pending_incoming_messages
            .push_back((connection_id, message.inner));
    }

    /// Returns the next event produced by the service. Returns `None` if no event is available.
    ///
    /// Call this function in a loop after having injected messages using
    /// [`Network::inject_connection_message`].
    pub fn next_event(&mut self) -> Option<Event<TConn>> {
        loop {
            // When a connection starts its shutdown, its id is put in `shutting_down_connection`.
            // When that happens, we go through the local state and clean up all requests and
            // notification substreams that are in progress/open and return the cancellations
            // as events.
            //
            // `shutting_down_connection` is set back to `None` only if it turns out that there
            // is no request or notification substream in progress/open anymore.
            if let Some(shutting_down_connection) = self.shutting_down_connection {
                // Outgoing notification substreams to close.
                for (_, substream_id) in self
                    .outgoing_notification_substreams_by_connection
                    .range(
                        (shutting_down_connection, SubstreamId::min_value())
                            ..=(shutting_down_connection, SubstreamId::max_value()),
                    )
                    .cloned()
                {
                    self.outgoing_notification_substreams_by_connection
                        .remove(&(shutting_down_connection, substream_id));
                    self.outgoing_notification_substreams.remove(&substream_id);
                    return Some(Event::NotificationsOutReset { substream_id });
                }

                // Ingoing notification substreams to close.
                for (key, substream_id) in self
                    .ingoing_notification_substreams_by_connection
                    .range(
                        (
                            shutting_down_connection,
                            established::SubstreamId::min_value(),
                        )
                            ..=(
                                shutting_down_connection,
                                established::SubstreamId::max_value(),
                            ),
                    )
                    .map(|(k, v)| (*k, *v))
                {
                    self.ingoing_notification_substreams
                        .remove(&substream_id)
                        .unwrap();
                    self.ingoing_notification_substreams_by_connection
                        .remove(&key)
                        .unwrap();

                    return Some(Event::NotificationsInClose {
                        substream_id,
                        outcome: Err(NotificationsInClosedErr::ConnectionShutdown),
                    });
                }

                // Find outgoing requests to cancel.
                for (_, substream_id) in self.outgoing_requests_by_connection.range(
                    (shutting_down_connection, SubstreamId::min_value())
                        ..=(shutting_down_connection, SubstreamId::max_value()),
                ) {
                    let _connection_id = self.outgoing_requests.remove(substream_id).unwrap();
                    debug_assert_eq!(_connection_id, shutting_down_connection);
                    let substream_id = *substream_id;
                    self.outgoing_requests_by_connection
                        .remove(&(shutting_down_connection, substream_id));

                    return Some(Event::Response {
                        substream_id,
                        response: Err(RequestError::ConnectionShutdown),
                    });
                }

                // Find ingoing requests to cancel.
                for (_, substream_id) in self.ingoing_requests_by_connection.range(
                    (shutting_down_connection, SubstreamId::min_value())
                        ..=(shutting_down_connection, SubstreamId::max_value()),
                ) {
                    let substream_id = *substream_id;

                    let _was_in = self.ingoing_requests.remove(&substream_id);
                    debug_assert!(_was_in.is_some());
                    let _was_in = self
                        .ingoing_requests_by_connection
                        .remove(&(shutting_down_connection, substream_id));
                    debug_assert!(_was_in);

                    return Some(Event::RequestInCancel { substream_id });
                }

                // If this is reached, this connection has no more request or notifications
                // substream that is still in progress or open. The connection is successfully
                // shut down.
                self.shutting_down_connection = None;
            }

            // Now actually process messages.
            let (connection_id, message) = self.pending_incoming_messages.pop_front()?;
            let connection = &mut self.connections.get_mut(&connection_id).unwrap();

            break Some(match message {
                ConnectionToCoordinatorInner::StartShutdown => {
                    connection.shutting_down = true;
                    debug_assert!(self.shutting_down_connection.is_none());
                    self.shutting_down_connection = Some(connection_id);
                    if connection.start_shutdown_called {
                        // No `StartShutdown` event is generated if the API user has started
                        // the shutdown themselves. In that case, `StartShutdown` is merely a
                        // confirmation.
                        continue;
                    } else {
                        Event::StartShutdown { id: connection_id }
                    }
                }
                ConnectionToCoordinatorInner::ShutdownFinished => {
                    debug_assert!(connection.shutting_down);
                    let user_data = self.connections.remove(&connection_id).unwrap().user_data;
                    self.messages_to_connections.push_back((
                        connection_id,
                        CoordinatorToConnectionInner::ShutdownFinishedAck,
                    ));
                    Event::Shutdown {
                        id: connection_id,
                        user_data,
                    }
                }
                ConnectionToCoordinatorInner::HandshakeFinished(peer_id) => {
                    debug_assert!(!connection.shutting_down);
                    debug_assert_eq!(
                        self.ingoing_notification_substreams_by_connection
                            .range(
                                (connection_id, established::SubstreamId::min_value())
                                    ..=(connection_id, established::SubstreamId::max_value())
                            )
                            .count(),
                        0
                    );
                    debug_assert_eq!(
                        self.outgoing_notification_substreams_by_connection
                            .range(
                                (connection_id, SubstreamId::min_value())
                                    ..=(connection_id, SubstreamId::max_value())
                            )
                            .count(),
                        0
                    );
                    debug_assert_eq!(
                        self.ingoing_requests_by_connection
                            .range(
                                (connection_id, SubstreamId::min_value())
                                    ..=(connection_id, SubstreamId::max_value())
                            )
                            .count(),
                        0
                    );

                    if connection.start_shutdown_called {
                        continue;
                    }

                    connection.handshaking = false;

                    Event::HandshakeFinished {
                        id: connection_id,
                        peer_id,
                    }
                }
                ConnectionToCoordinatorInner::InboundError(error) => {
                    debug_assert!(!connection.shutting_down);
                    if connection.start_shutdown_called {
                        continue;
                    }

                    Event::InboundError {
                        id: connection_id,
                        error,
                    }
                }
                ConnectionToCoordinatorInner::RequestIn {
                    id: connection_substream_id,
                    protocol_index,
                    request,
                } => {
                    debug_assert!(!connection.shutting_down);
                    if connection.start_shutdown_called {
                        continue;
                    }

                    let substream_id = self.next_substream_id;
                    self.next_substream_id.0 += 1;

                    self.ingoing_requests
                        .insert(substream_id, (connection_id, connection_substream_id));
                    self.ingoing_requests_by_connection
                        .insert((connection_id, substream_id));

                    Event::RequestIn {
                        id: connection_id,
                        substream_id,
                        protocol_index,
                        request_payload: request,
                    }
                }
                ConnectionToCoordinatorInner::Response {
                    id: substream_id,
                    response,
                    ..
                } => {
                    debug_assert!(!connection.shutting_down);
                    if connection.start_shutdown_called {
                        continue;
                    }

                    let _was_in = self.outgoing_requests.remove(&substream_id).unwrap();
                    debug_assert_eq!(_was_in, connection_id);
                    let _was_in = self
                        .outgoing_requests_by_connection
                        .remove(&(connection_id, substream_id));
                    debug_assert!(_was_in);

                    Event::Response {
                        substream_id,
                        response: response.map_err(RequestError::Substream),
                    }
                }
                ConnectionToCoordinatorInner::NotificationsInOpen {
                    id: inner_substream_id,
                    protocol_index: overlay_network_index,
                    handshake,
                } => {
                    debug_assert!(!connection.shutting_down);
                    if connection.start_shutdown_called {
                        continue;
                    }

                    let substream_id = self.next_substream_id;
                    self.next_substream_id.0 += 1;

                    self.ingoing_notification_substreams.insert(
                        substream_id,
                        (connection_id, InSubstreamState::Pending, inner_substream_id),
                    );
                    self.ingoing_notification_substreams_by_connection
                        .insert((connection_id, inner_substream_id), substream_id);

                    Event::NotificationsInOpen {
                        id: connection_id,
                        substream_id,
                        notifications_protocol_index: overlay_network_index,
                        remote_handshake: handshake,
                    }
                }
                ConnectionToCoordinatorInner::NotificationsInOpenCancel {
                    id: inner_substream_id,
                    ..
                } => {
                    debug_assert!(!connection.shutting_down);
                    if connection.start_shutdown_called {
                        continue;
                    }

                    let substream_id = self
                        .ingoing_notification_substreams_by_connection
                        .remove(&(connection_id, inner_substream_id))
                        .unwrap();
                    let _was_in = self.ingoing_notification_substreams.remove(&substream_id);
                    debug_assert!(_was_in.is_some());

                    Event::NotificationsInClose {
                        substream_id,
                        outcome: Err(NotificationsInClosedErr::Substream(
                            established::NotificationsInClosedErr::SubstreamReset,
                        )),
                    }
                }
                ConnectionToCoordinatorInner::NotificationIn {
                    id: inner_substream_id,
                    notification,
                } => {
                    debug_assert!(!connection.shutting_down);
                    if connection.start_shutdown_called {
                        continue;
                    }

                    let substream_id = *self
                        .ingoing_notification_substreams_by_connection
                        .get(&(connection_id, inner_substream_id))
                        .unwrap();

                    Event::NotificationsIn {
                        substream_id,
                        notification,
                    }
                }
                ConnectionToCoordinatorInner::NotificationsInClose {
                    id: inner_substream_id,
                    outcome,
                    ..
                } => {
                    debug_assert!(!connection.shutting_down);
                    if connection.start_shutdown_called {
                        continue;
                    }

                    let substream_id = self
                        .ingoing_notification_substreams_by_connection
                        .remove(&(connection_id, inner_substream_id))
                        .unwrap();
                    let _was_in = self.ingoing_notification_substreams.remove(&substream_id);
                    debug_assert!(_was_in.is_some());

                    Event::NotificationsInClose {
                        substream_id,
                        outcome: outcome.map_err(NotificationsInClosedErr::Substream),
                    }
                }
                ConnectionToCoordinatorInner::NotificationsOutResult {
                    id: substream_id,
                    result,
                } => {
                    debug_assert!(!connection.shutting_down);
                    if connection.start_shutdown_called {
                        continue;
                    }

                    let mut entry = match self.outgoing_notification_substreams.entry(substream_id)
                    {
                        hashbrown::hash_map::Entry::Occupied(e) => e,
                        hashbrown::hash_map::Entry::Vacant(_) => unreachable!(),
                    };

                    debug_assert!(matches!(entry.get_mut().1, OutSubstreamState::Pending));

                    if result.is_ok() {
                        entry.insert((connection_id, OutSubstreamState::Open));
                    } else {
                        entry.remove();

                        let _was_removed = self
                            .outgoing_notification_substreams_by_connection
                            .remove(&(connection_id, substream_id));
                        debug_assert!(_was_removed);
                    }

                    Event::NotificationsOutResult {
                        substream_id,
                        result,
                    }
                }
                ConnectionToCoordinatorInner::NotificationsOutCloseDemanded {
                    id: substream_id,
                    ..
                } => {
                    debug_assert!(!connection.shutting_down);
                    if connection.start_shutdown_called {
                        continue;
                    }

                    Event::NotificationsOutCloseDemanded { substream_id }
                }
                ConnectionToCoordinatorInner::NotificationsOutReset { id: substream_id } => {
                    debug_assert!(!connection.shutting_down);
                    if connection.start_shutdown_called {
                        continue;
                    }

                    let _was_in = self.outgoing_notification_substreams.remove(&substream_id);
                    debug_assert!(_was_in.is_some());

                    let _was_removed = self
                        .outgoing_notification_substreams_by_connection
                        .remove(&(connection_id, substream_id));
                    debug_assert!(_was_removed);

                    Event::NotificationsOutReset { substream_id }
                }
                ConnectionToCoordinatorInner::PingOutSuccess => {
                    debug_assert!(!connection.shutting_down);
                    if connection.start_shutdown_called {
                        continue;
                    }

                    Event::PingOutSuccess { id: connection_id }
                }
                ConnectionToCoordinatorInner::PingOutFailed => {
                    debug_assert!(!connection.shutting_down);
                    if connection.start_shutdown_called {
                        continue;
                    }

                    Event::PingOutFailed { id: connection_id }
                }
            });
        }
    }
}

impl<TConn, TNow> ops::Index<ConnectionId> for Network<TConn, TNow> {
    type Output = TConn;
    fn index(&self, id: ConnectionId) -> &TConn {
        &self.connections.get(&id).unwrap().user_data
    }
}

impl<TConn, TNow> ops::IndexMut<ConnectionId> for Network<TConn, TNow> {
    fn index_mut(&mut self, id: ConnectionId) -> &mut TConn {
        &mut self.connections.get_mut(&id).unwrap().user_data
    }
}

/// State machine dedicated to a single connection.
pub struct ConnectionTask<TNow> {
    /// State machine of the underlying connection.
    connection: ConnectionInner<TNow>,

    /// Buffer of messages destined to the coordinator.
    ///
    /// Never goes above a few elements.
    pending_messages: VecDeque<ConnectionToCoordinatorInner>,
}

enum ConnectionInner<TNow> {
    /// Connection is still in its handshake phase.
    Handshake {
        handshake: handshake::HealthyHandshake,

        /// Seed that will be used to initialize randomness when building the
        /// [`established::Established`].
        /// This seed is computed during the handshake in order to avoid having to access a shared
        /// state when the handshake is over. While it seems a bit dangerous to leave a randomness
        /// seed in plain memory, the randomness isn't used for anything critical or related to
        /// cryptography, but only for example to avoid hash collision attacks.
        randomness_seed: [u8; 16],

        /// When the handshake phase times out.
        timeout: TNow,

        /// See [`Config::noise_key`].
        noise_key: Arc<NoiseKey>,

        /// See [`OverlayNetwork`].
        notification_protocols: Arc<[OverlayNetwork]>,

        /// See [`Config::request_response_protocols`].
        request_response_protocols: Arc<[ConfigRequestResponse]>,

        /// See [`Config::ping_protocol`].
        ping_protocol: Arc<str>,
    },

    /// Connection has been fully established.
    Established {
        // TODO: user data of request redundant with the substreams mapping below
        established: established::Established<TNow, SubstreamId, ()>,

        /// Because outgoing substream ids are assigned by the coordinator, we maintain a mapping
        /// of the "outer ids" to "inner ids".
        outbound_substreams_mapping:
            hashbrown::HashMap<SubstreamId, established::SubstreamId, fnv::FnvBuildHasher>,

        /// Reverse mapping.
        // TODO: could be user datas in established?
        outbound_substreams_mapping2:
            hashbrown::HashMap<established::SubstreamId, SubstreamId, fnv::FnvBuildHasher>,
    },

    /// Connection has finished its shutdown. A [`CoordinatorToConnectionInner::ShutdownFinished`]
    /// message has been sent and is waiting to be acknowledged.
    ShutdownWaitingAck {
        /// If true, [`ConnectionTask::reset`] has been called. This doesn't modify any of the
        /// behavior but is used to make sure that the API is used correctly.
        was_api_reset: bool,
    },

    /// Connection has finished its shutdown and its shutdown has been acknowledged. There is
    /// nothing more to do except stop the connection task.
    ShutdownAcked {
        /// If true, [`ConnectionTask::reset`] has been called. This doesn't modify any of the
        /// behavior but is used to make sure that the API is used correctly.
        was_api_reset: bool,
    },

    /// Temporary state used to statisfy the borrow checker during state transitions.
    Poisoned,
}

impl<TNow> ConnectionTask<TNow>
where
    TNow: Clone + Add<Duration, Output = TNow> + Sub<TNow, Output = Duration> + Ord,
{
    /// Pulls a message to send back to the coordinator.
    ///
    /// This function takes ownership of `self` and optionally yields it back. If the first
    /// option contains `None`, then no more message will be generated and the [`ConnectionTask`]
    /// has vanished. This will happen after the connection has been shut down or reset.
    /// It is possible for `self` to not be yielded back even if the [`ReadWrite`] that was last
    /// passed to [`ConnectionTask::read_write`] is still fully open, in which case the API user
    /// should abruptly reset the connection, for example by sending a TCP RST flag. This can
    /// happen for exemple if the connection seems unresponsive and that an attempt at closing
    /// the connection in a clean way is futile.
    ///
    /// If any message is returned, it is the responsibility of the API user to send it to the
    /// coordinator by calling [`Network::inject_connection_message`].
    /// Do not attempt to buffer the message being returned, as it would work against the
    /// back-pressure strategy used internally. As soon as a message is returned, it should be
    /// delivered. If the coordinator is busy at the moment a message should be delivered, then
    /// the entire thread of execution dedicated to this [`ConnectionTask`] should be paused until
    /// the coordinator is ready and the message delivered.
    ///
    /// Messages aren't generated spontaneously. In other words, you don't need to periodically
    /// call this function just in case there's a new message. Messages are always generated after
    /// either [`ConnectionTask::read_write`] or [`ConnectionTask::reset`] has been called.
    /// Multiple messages can happen in a row.
    ///
    /// Because this function frees space in a buffer, calling [`ConnectionTask::read_write`]
    /// again after it has returned might read/write more data and generate an event again. In
    /// other words, the API user should call [`ConnectionTask::read_write`] and
    /// [`ConnectionTask::pull_message_to_coordinator`] repeatedly in a loop until no more
    /// message is generated.
    pub fn pull_message_to_coordinator(
        mut self,
    ) -> (Option<Self>, Option<ConnectionToCoordinator>) {
        // To be sure that there is no bug in the implementation, we make sure that the number of
        // buffered messages doesn't go above a certain small limit.
        debug_assert!(self.pending_messages.len() < 8);

        let message = self
            .pending_messages
            .pop_front()
            .map(|inner| ConnectionToCoordinator { inner });

        // The `ShutdownAcked` state causes the task to exit.
        let self_ret = if !matches!(self.connection, ConnectionInner::ShutdownAcked { .. }) {
            Some(self)
        } else {
            None
        };

        (self_ret, message)
    }

    /// Injects a message that has been pulled using [`Network::pull_message_to_connection`].
    ///
    /// Calling this function might generate data to send to the connection. You should call
    /// [`ConnectionTask::read_write`] after this function has returned (unless you have called
    /// [`ConnectionTask::reset`] in the past).
    pub fn inject_coordinator_message(&mut self, message: CoordinatorToConnection<TNow>) {
        match (message.inner, &mut self.connection) {
            (
                CoordinatorToConnectionInner::StartRequest {
                    request_data,
                    timeout,
                    protocol_index,
                    substream_id,
                },
                ConnectionInner::Established {
                    established,
                    outbound_substreams_mapping,
                    outbound_substreams_mapping2,
                },
            ) => {
                let inner_substream_id =
                    established.add_request(protocol_index, request_data, timeout, substream_id);
                let _prev_value =
                    outbound_substreams_mapping.insert(substream_id, inner_substream_id);
                debug_assert!(_prev_value.is_none());
                let _prev_value =
                    outbound_substreams_mapping2.insert(inner_substream_id, substream_id);
                debug_assert!(_prev_value.is_none());
            }
            (
                CoordinatorToConnectionInner::OpenOutNotifications {
                    handshake,
                    now,
                    overlay_network_index,
                    substream_id: outer_substream_id,
                },
                ConnectionInner::Established {
                    established,
                    outbound_substreams_mapping,
                    outbound_substreams_mapping2,
                },
            ) => {
                let inner_substream_id = established.open_notifications_substream(
                    now,
                    overlay_network_index,
                    handshake,
                    (),
                );

                let _prev_value =
                    outbound_substreams_mapping.insert(outer_substream_id, inner_substream_id);
                debug_assert!(_prev_value.is_none());
                let _prev_value =
                    outbound_substreams_mapping2.insert(inner_substream_id, outer_substream_id);
                debug_assert!(_prev_value.is_none());
            }
            (
                CoordinatorToConnectionInner::CloseOutNotifications { substream_id },
                ConnectionInner::Established {
                    established,
                    outbound_substreams_mapping,
                    outbound_substreams_mapping2,
                },
            ) => {
                let inner_substream_id = outbound_substreams_mapping.remove(&substream_id).unwrap();
                outbound_substreams_mapping2.remove(&inner_substream_id);
                established.close_notifications_substream(inner_substream_id);
            }
            (
                CoordinatorToConnectionInner::QueueNotification {
                    substream_id,
                    notification,
                },
                ConnectionInner::Established {
                    established,
                    outbound_substreams_mapping,
                    ..
                },
            ) => {
                // It is possible that the remote has closed the outbound notification substream
                // while a `QueueNotification` message was being delivered, or that the API user
                // queued a notification before the message about the substream being closed was
                // delivered to the coordinator.
                // If that happens, we intentionally silently discard the message, causing the
                // notification to not be sent. This is consistent with the guarantees about
                // notifications delivered that are documented in the public API.
                if let Some(inner_substream_id) = outbound_substreams_mapping.get(&substream_id) {
                    established.write_notification_unbounded(*inner_substream_id, notification);
                }
            }
            (
                CoordinatorToConnectionInner::AnswerRequest {
                    substream_id,
                    response,
                },
                ConnectionInner::Established { established, .. },
            ) => match established.respond_in_request(substream_id, response) {
                Ok(()) => {}
                Err(established::RespondInRequestError::SubstreamClosed) => {
                    // As documented, answering an obsolete request is simply ignored.
                }
            },
            (
                CoordinatorToConnectionInner::AcceptInNotifications {
                    substream_id,
                    handshake,
                },
                ConnectionInner::Established { established, .. },
            ) => {
                established.accept_in_notifications_substream(substream_id, handshake, ());
            }
            (
                CoordinatorToConnectionInner::RejectInNotifications { substream_id },
                ConnectionInner::Established { established, .. },
            ) => {
                established.reject_in_notifications_substream(substream_id);
            }
            (
                CoordinatorToConnectionInner::StartShutdown { .. },
                ConnectionInner::Established { .. } | ConnectionInner::Handshake { .. },
            ) => {
                // TODO: implement proper shutdown
                self.pending_messages
                    .push_back(ConnectionToCoordinatorInner::StartShutdown);
                self.pending_messages
                    .push_back(ConnectionToCoordinatorInner::ShutdownFinished);
                self.connection = ConnectionInner::ShutdownWaitingAck {
                    was_api_reset: false,
                };
            }
            (
                CoordinatorToConnectionInner::AcceptInNotifications { .. }
                | CoordinatorToConnectionInner::RejectInNotifications { .. }
                | CoordinatorToConnectionInner::AnswerRequest { .. }
                | CoordinatorToConnectionInner::OpenOutNotifications { .. }
                | CoordinatorToConnectionInner::CloseOutNotifications { .. }
                | CoordinatorToConnectionInner::QueueNotification { .. },
                ConnectionInner::Handshake { .. } | ConnectionInner::ShutdownAcked { .. },
            ) => unreachable!(),
            (
                CoordinatorToConnectionInner::AcceptInNotifications { .. }
                | CoordinatorToConnectionInner::RejectInNotifications { .. }
                | CoordinatorToConnectionInner::AnswerRequest { .. }
                | CoordinatorToConnectionInner::OpenOutNotifications { .. }
                | CoordinatorToConnectionInner::CloseOutNotifications { .. }
                | CoordinatorToConnectionInner::QueueNotification { .. },
                ConnectionInner::ShutdownWaitingAck { .. },
            ) => {
                // There might still be some messages coming from the coordinator after the
                // connection task has sent a message indicating that it has shut down. This is
                // due to the concurrent nature of the API and doesn't indicate a bug. These
                // messages are simply ignored by the connection task.
            }
            (
                CoordinatorToConnectionInner::ShutdownFinishedAck,
                ConnectionInner::ShutdownWaitingAck {
                    was_api_reset: was_reset,
                },
            ) => {
                self.connection = ConnectionInner::ShutdownAcked {
                    was_api_reset: *was_reset,
                };
            }
            _ => todo!(), // TODO:
        }
    }

    /// Sets the state of the connection to "reset".
    ///
    /// This should be called if the remote abruptly closes the connection, such as with a TCP/IP
    /// RST flag.
    ///
    /// After this function has been called, it is illegal to call [`ConnectionTask::read_write`]
    /// or [`ConnectionTask::reset`] again.
    ///
    /// Calling this function might have generated messages for the coordinator.
    /// [`ConnectionTask::pull_message_to_coordinator`] should be called afterwards in order to
    /// process these messages.
    ///
    /// # Panic
    ///
    /// Panics if [`ConnectionTask::reset`] has been called in the past.
    ///
    pub fn reset(&mut self) {
        // It is illegal to call `reset` a second time. Verify that the user didn't do this.
        if let ConnectionInner::ShutdownWaitingAck {
            was_api_reset: true,
        }
        | ConnectionInner::ShutdownAcked {
            was_api_reset: true,
        } = self.connection
        {
            panic!()
        }

        self.pending_messages
            .push_back(ConnectionToCoordinatorInner::StartShutdown);
        self.pending_messages
            .push_back(ConnectionToCoordinatorInner::ShutdownFinished);
        self.connection = ConnectionInner::ShutdownWaitingAck {
            was_api_reset: true,
        };
    }

    /// Reads data coming from the connection, updates the internal state machine, and writes data
    /// destined to the connection through the [`ReadWrite`].
    ///
    /// Calling this function might have generated messages for the coordinator.
    /// [`ConnectionTask::pull_message_to_coordinator`] should be called afterwards in order to
    /// process these messages.
    ///
    /// # Panic
    ///
    /// Panics if [`ConnectionTask::reset`] has been called in the past.
    ///
    pub fn read_write(&mut self, read_write: &'_ mut ReadWrite<'_, TNow>) {
        // There is already at least one pending message. We back-pressure the connection by not
        // performing any reading or writing, as this might generate more messages and open the
        // door for a DoS attack by the remote. As documented, the API user is supposed to pull
        // messages after this function has returned, meaning that they will drain the messages.
        if !self.pending_messages.is_empty() {
            return;
        }

        match mem::replace(&mut self.connection, ConnectionInner::Poisoned) {
            ConnectionInner::Established {
                established,
                mut outbound_substreams_mapping,
                mut outbound_substreams_mapping2,
            } => match established.read_write(read_write) {
                Ok((connection, event)) => {
                    if read_write.is_dead() && event.is_none() {
                        // TODO: provide error
                        self.pending_messages
                            .push_back(ConnectionToCoordinatorInner::StartShutdown);
                        self.pending_messages
                            .push_back(ConnectionToCoordinatorInner::ShutdownFinished);
                        self.connection = ConnectionInner::ShutdownWaitingAck {
                            was_api_reset: false,
                        };
                        return;
                    }

                    match event {
                        Some(established::Event::InboundError(err)) => {
                            self.pending_messages
                                .push_back(ConnectionToCoordinatorInner::InboundError(err));
                        }
                        Some(established::Event::RequestIn {
                            id,
                            protocol_index,
                            request,
                        }) => {
                            self.pending_messages.push_back(
                                ConnectionToCoordinatorInner::RequestIn {
                                    id,
                                    protocol_index,
                                    request,
                                },
                            );
                        }
                        Some(established::Event::Response { id, response, .. }) => {
                            let outer_substream_id =
                                outbound_substreams_mapping2.remove(&id).unwrap();
                            outbound_substreams_mapping
                                .remove(&outer_substream_id)
                                .unwrap();
                            self.pending_messages.push_back(
                                ConnectionToCoordinatorInner::Response {
                                    response,
                                    id: outer_substream_id,
                                },
                            );
                        }
                        Some(established::Event::NotificationsInOpen {
                            id,
                            protocol_index,
                            handshake,
                        }) => {
                            self.pending_messages.push_back(
                                ConnectionToCoordinatorInner::NotificationsInOpen {
                                    id,
                                    protocol_index,
                                    handshake,
                                },
                            );
                        }
                        Some(established::Event::NotificationsInOpenCancel { id, .. }) => {
                            self.pending_messages.push_back(
                                ConnectionToCoordinatorInner::NotificationsInOpenCancel { id },
                            );
                        }
                        Some(established::Event::NotificationIn { id, notification }) => {
                            self.pending_messages.push_back(
                                ConnectionToCoordinatorInner::NotificationIn { id, notification },
                            );
                        }
                        Some(established::Event::NotificationsInClose { id, outcome, .. }) => {
                            self.pending_messages.push_back(
                                ConnectionToCoordinatorInner::NotificationsInClose { id, outcome },
                            );
                        }
                        Some(established::Event::NotificationsOutResult { id, result }) => {
                            let outer_substream_id =
                                *outbound_substreams_mapping2.get(&id).unwrap();

                            if result.is_err() {
                                outbound_substreams_mapping.remove(&outer_substream_id);
                                outbound_substreams_mapping2.remove(&id);
                            }

                            self.pending_messages.push_back(
                                ConnectionToCoordinatorInner::NotificationsOutResult {
                                    id: outer_substream_id,
                                    result: result.map_err(|(err, _)| err),
                                },
                            );
                        }
                        Some(established::Event::NotificationsOutCloseDemanded { id }) => {
                            let outer_substream_id =
                                *outbound_substreams_mapping2.get(&id).unwrap();
                            self.pending_messages.push_back(
                                ConnectionToCoordinatorInner::NotificationsOutCloseDemanded {
                                    id: outer_substream_id,
                                },
                            );
                        }
                        Some(established::Event::NotificationsOutReset { id, .. }) => {
                            let outer_substream_id =
                                outbound_substreams_mapping2.remove(&id).unwrap();
                            outbound_substreams_mapping.remove(&outer_substream_id);
                            self.pending_messages.push_back(
                                ConnectionToCoordinatorInner::NotificationsOutReset {
                                    id: outer_substream_id,
                                },
                            );
                        }
                        Some(established::Event::PingOutSuccess) => {
                            self.pending_messages
                                .push_back(ConnectionToCoordinatorInner::PingOutSuccess);
                        }
                        Some(established::Event::PingOutFailed) => {
                            self.pending_messages
                                .push_back(ConnectionToCoordinatorInner::PingOutFailed);
                        }
                        None => {}
                    }

                    self.connection = ConnectionInner::Established {
                        established: connection,
                        outbound_substreams_mapping,
                        outbound_substreams_mapping2,
                    };
                }
                Err(_err) => {
                    // TODO: provide error
                    self.pending_messages
                        .push_back(ConnectionToCoordinatorInner::StartShutdown);
                    self.pending_messages
                        .push_back(ConnectionToCoordinatorInner::ShutdownFinished);
                    self.connection = ConnectionInner::ShutdownWaitingAck {
                        was_api_reset: false,
                    };
                }
            },

            ConnectionInner::Handshake {
                mut handshake,
                randomness_seed,
                timeout,
                noise_key,
                notification_protocols,
                request_response_protocols,
                ping_protocol,
            } => {
                // Check that the handshake isn't taking too long.
                //
                // Note that we check this condition before looking into the incoming data,
                // and it is possible for the buffers to contain the data that leads to the
                // handshake being finished. If that is the case, however, it is impossible to
                // know whether this data arrived before or after the timeout.
                // Whether to put this check before or after reading the buffer is a choice
                // between having false negatives or having false positives for the timeout.
                // We are more strict than necessary by having the check before, but this
                // guarantees that no horrendously slow connections can accidentally make their
                // way through.
                if timeout < read_write.now {
                    // TODO: provide error: ConnectionError::Handshake(HandshakeError::Timeout)
                    self.pending_messages
                        .push_back(ConnectionToCoordinatorInner::StartShutdown);
                    self.pending_messages
                        .push_back(ConnectionToCoordinatorInner::ShutdownFinished);
                    self.connection = ConnectionInner::ShutdownWaitingAck {
                        was_api_reset: false,
                    };
                    return;
                }

                // `read_write()` should be called again as soon as possible after `timeout` in
                // order for the check above to work.
                read_write.wake_up_after(&timeout);

                loop {
                    let (read_before, written_before) =
                        (read_write.read_bytes, read_write.written_bytes);

                    let result = match handshake.read_write(read_write) {
                        Ok(rw) => rw,
                        Err(_err) => {
                            // TODO: provide error: ConnectionError::Handshake(HandshakeError::Protocol(err))
                            self.pending_messages
                                .push_back(ConnectionToCoordinatorInner::StartShutdown);
                            self.pending_messages
                                .push_back(ConnectionToCoordinatorInner::ShutdownFinished);
                            self.connection = ConnectionInner::ShutdownWaitingAck {
                                was_api_reset: false,
                            };
                            return;
                        }
                    };

                    match result {
                        handshake::Handshake::Healthy(updated_handshake)
                            if (read_before, written_before)
                                == (read_write.read_bytes, read_write.written_bytes) =>
                        {
                            self.connection = ConnectionInner::Handshake {
                                handshake: updated_handshake,
                                randomness_seed,
                                timeout,
                                noise_key,
                                notification_protocols,
                                request_response_protocols,
                                ping_protocol,
                            };
                            break;
                        }
                        handshake::Handshake::Healthy(updated_handshake) => {
                            handshake = updated_handshake;
                        }
                        handshake::Handshake::Success {
                            remote_peer_id,
                            connection,
                        } => {
                            self.pending_messages.push_back(
                                ConnectionToCoordinatorInner::HandshakeFinished(remote_peer_id),
                            );

                            self.connection = ConnectionInner::Established {
                                established: connection.into_connection(established::Config {
                                    notifications_protocols: notification_protocols
                                        .iter()
                                        .flat_map(|net| {
                                            let max_handshake_size = net.config.max_handshake_size;
                                            let max_notification_size =
                                                net.config.max_notification_size;
                                            iter::once(&net.config.protocol_name)
                                                .chain(net.config.fallback_protocol_names.iter())
                                                .map(move |name| {
                                                    established::ConfigNotifications {
                                                        name: name.clone(), // TODO: cloning :-/
                                                        max_handshake_size,
                                                        max_notification_size,
                                                    }
                                                })
                                        })
                                        .collect(),
                                    request_protocols: request_response_protocols.to_vec(), // TODO: overhead
                                    randomness_seed,
                                    ping_protocol: ping_protocol.to_string(), // TODO: cloning :-/
                                    ping_interval: Duration::from_secs(20),   // TODO: hardcoded
                                    ping_timeout: Duration::from_secs(10),    // TODO: hardcoded
                                    first_out_ping: read_write.now.clone() + Duration::from_secs(2), // TODO: hardcoded
                                }),
                                outbound_substreams_mapping:
                                    hashbrown::HashMap::with_capacity_and_hasher(
                                        0,
                                        Default::default(),
                                    ), // TODO: capacity?
                                outbound_substreams_mapping2:
                                    hashbrown::HashMap::with_capacity_and_hasher(
                                        0,
                                        Default::default(),
                                    ), // TODO: capacity?
                            };
                            break;
                        }
                        handshake::Handshake::NoiseKeyRequired(key) => {
                            handshake = key.resume(&noise_key);
                        }
                    }
                }
            }

            c @ (ConnectionInner::ShutdownWaitingAck {
                was_api_reset: false,
            }
            | ConnectionInner::ShutdownAcked {
                was_api_reset: false,
            }) => {
                // The user might legitimately call this function after the connection has
                // already shut down. In that case, just do nothing.
                self.connection = c;

                // This might have been done already during the shutdown process, but we do it
                // again just in case.
                read_write.close_write();
            }

            ConnectionInner::ShutdownWaitingAck {
                was_api_reset: true,
            }
            | ConnectionInner::ShutdownAcked {
                was_api_reset: true,
            } => {
                // As documented, it is illegal to call this function after calling `reset()`.
                panic!()
            }

            ConnectionInner::Poisoned => unreachable!(),
        }
    }
}

pub struct ConnectionToCoordinator {
    inner: ConnectionToCoordinatorInner,
}

enum ConnectionToCoordinatorInner {
    HandshakeFinished(PeerId),

    /// See the corresponding event in [`established::Event`].
    InboundError(established::InboundError),

    /// See the corresponding event in [`established::Event`].
    RequestIn {
        id: established::SubstreamId,
        protocol_index: usize,
        request: Vec<u8>,
    },

    /// See the corresponding event in [`established::Event`].
    Response {
        response: Result<Vec<u8>, established::RequestError>,
        id: SubstreamId,
    },

    /// See the corresponding event in [`established::Event`].
    NotificationsInOpen {
        id: established::SubstreamId,
        protocol_index: usize,
        handshake: Vec<u8>,
    },
    /// See the corresponding event in [`established::Event`].
    NotificationsInOpenCancel {
        id: established::SubstreamId,
    },
    /// See the corresponding event in [`established::Event`].
    NotificationIn {
        id: established::SubstreamId,
        notification: Vec<u8>,
    },
    /// See the corresponding event in [`established::Event`].
    NotificationsInClose {
        id: established::SubstreamId,
        outcome: Result<(), established::NotificationsInClosedErr>,
    },
    /// See the corresponding event in [`established::Event`].
    NotificationsOutResult {
        id: SubstreamId,
        result: Result<Vec<u8>, established::NotificationsOutErr>,
    },
    /// See the corresponding event in [`established::Event`].
    NotificationsOutCloseDemanded {
        id: SubstreamId,
    },
    /// See the corresponding event in [`established::Event`].
    NotificationsOutReset {
        id: SubstreamId,
    },
    /// See the corresponding event in [`established::Event`].
    PingOutSuccess,
    /// See the corresponding event in [`established::Event`].
    PingOutFailed,

    /// Sent either in response to [`CoordinatorToConnectionInner::StartShutdown`] or if the
    /// remote has initiated the shutdown. After this, no more [`CoordinatorToConnectionInner`]
    /// will be sent anymore except for [`CoordinatorToConnectionInner::ShutdownFinished`].
    StartShutdown,

    /// Shutdown has now finished. Always sent after
    /// [`ConnectionToCoordinatorInner::StartShutdown`]. No message is sent by the connection
    /// task anymore after that.
    ///
    /// Must be confirmed with a [`CoordinatorToConnectionInner::ShutdownFinishedAck`].
    ShutdownFinished,
}

pub struct CoordinatorToConnection<TNow> {
    inner: CoordinatorToConnectionInner<TNow>,
}

enum CoordinatorToConnectionInner<TNow> {
    /// Connection task must terminate. This is always sent back after a
    /// [`ConnectionToCoordinatorInner::ShutdownFinished`].
    ///
    /// This final message is necessary in order to make sure that the coordinator doesn't
    /// generate messages destined to a connection that isn't alive anymore.
    ShutdownFinishedAck,

    /// Connection must start shutting down if it is not already the case.
    /// Before of concurrency, it is possible for this message to be sent/received *after* a
    /// [`ConnectionToCoordinatorInner::StartShutdown`] has been sent.
    StartShutdown,

    StartRequest {
        protocol_index: usize,
        request_data: Vec<u8>,
        timeout: TNow,
        /// Id of the substream assigned by the coordinator.
        /// This is **not** the same as the actual substream used in the connection.
        substream_id: SubstreamId,
    },
    OpenOutNotifications {
        /// Id of the substream assigned by the coordinator.
        /// This is **not** the same as the actual substream used in the connection.
        substream_id: SubstreamId,
        overlay_network_index: usize,
        now: TNow,
        handshake: Vec<u8>,
    },
    CloseOutNotifications {
        /// Id of the substream assigned by the coordinator.
        /// This is **not** the same as the actual substream used in the connection.
        substream_id: SubstreamId,
    },
    QueueNotification {
        /// Id of the substream assigned by the coordinator.
        /// This is **not** the same as the actual substream used in the connection.
        substream_id: SubstreamId,
        notification: Vec<u8>,
    },
    AcceptInNotifications {
        substream_id: established::SubstreamId,
        handshake: Vec<u8>,
    },
    RejectInNotifications {
        substream_id: established::SubstreamId,
    },

    /// Answer an incoming request.
    ///
    /// Since the API doesn't provide any feedback about whether responses have been successfully
    /// received by the remote, the response should simply be ignored in case the substream is
    /// obsolete. In any case, answering an obsolete request is not an API error because the remote
    /// might have cancelled their request while the message containing the response was waiting
    /// in queue.
    AnswerRequest {
        substream_id: established::SubstreamId,
        response: Result<Vec<u8>, ()>,
    },
}

/// Event generated by [`Network::next_event`].
#[derive(Debug)]
pub enum Event<TConn> {
    /// Handshake of the given connection has completed.
    ///
    /// This event can only happen once per connection.
    HandshakeFinished {
        /// Identifier of the connection whose handshake is finished.
        id: ConnectionId,
        /// Identity of the peer on the other side of the connection.
        peer_id: PeerId,
    },

    /// A transport-level connection (e.g. a TCP socket) is starting its shutdown.
    ///
    /// It is no longer possible to start requests, open notification substreams, or open
    /// notifications on this connection, and no new incoming requests or notification substreams
    /// will be reported as events.
    ///
    /// Keep in mind that this event can happen for connections that haven't finished their
    /// handshake.
    ///
    /// This event is **not** generated when [`Network::start_shutdown`] is called.
    // TODO: add reason for shutdown?
    StartShutdown { id: ConnectionId },

    /// A transport-level connection (e.g. a TCP socket) has been shut down.
    ///
    /// This [`ConnectionId`] is no longer valid, and using it will result in panics.
    // TODO: add reason for shutdown?
    Shutdown { id: ConnectionId, user_data: TConn },

    /// Received an incoming substream, but this substream has produced an error.
    ///
    /// > **Note**: This event exists only for diagnostic purposes. No action is expected in
    /// >           return.
    InboundError {
        id: ConnectionId,
        /// Error that happened.
        error: InboundError,
    },

    /// Outcome of a request started using [`Network::start_request`].
    ///
    /// *All* requests always lead to an outcome, even if the connection has been closed while the
    /// request was in progress.
    Response {
        substream_id: SubstreamId,
        response: Result<Vec<u8>, RequestError>,
    },

    /// Received a request from a request-response protocol.
    RequestIn {
        id: ConnectionId,
        /// Substream on which the request has been received. Must be passed back when providing
        /// the response.
        substream_id: SubstreamId,
        protocol_index: usize,
        request_payload: Vec<u8>,
    },

    /// Request received earlier has been cancelled by the remote.
    ///
    /// The [`SubstreamId`] is now invalid.
    RequestInCancel { substream_id: SubstreamId },

    /// Outcome of trying to open a substream with [`Network::open_notifications_substream`].
    ///
    /// If `Ok`, it is now possible to send notifications on this substream.
    /// If `Err`, the substream no longer exists and the [`SubstreamId`] becomes invalid.
    NotificationsOutResult {
        // TODO: what if fallback?
        substream_id: SubstreamId,
        /// If `Ok`, contains the handshake sent back by the remote. Its interpretation is out of
        /// scope of this module.
        result: Result<Vec<u8>, NotificationsOutErr>,
    },

    /// Remote has closed an outgoing notifications substream, meaning that it demands the closing
    /// of the substream. Use [`Network::close_out_notifications`] as soon as possible, which is
    /// typically after all outbound notifications that need to be queued have been queued.
    NotificationsOutCloseDemanded { substream_id: SubstreamId },

    /// A previously open outbound substream has been closed, by the remote or as a consequence of
    /// the connection shutting down.
    ///
    /// The substream no longer exists and the [`SubstreamId`] becomes invalid.
    // TODO: ambiguity with failed NotificationsOutResult
    NotificationsOutReset { substream_id: SubstreamId },

    /// The remote would like to open a notifications substream.
    ///
    /// The substream needs to be accepted or refused using [`Network::accept_notifications_in`]
    /// or [`Network::reject_notifications_in`].
    NotificationsInOpen {
        id: ConnectionId,
        /// Newly-generated identifier for the substream on which the request has been received.
        /// Must be passed back when accepting or refusing the substream.
        substream_id: SubstreamId,
        notifications_protocol_index: usize,
        remote_handshake: Vec<u8>,
    },

    /// Received a notification on a notifications substream of a connection.
    NotificationsIn {
        /// Substream on which the notification has been received. Guaranteed to be a substream
        /// that has been accepted with [`Network::accept_notifications_in`].
        substream_id: SubstreamId,
        /// Notification that the remote has sent. The meaning of this data is out of scope of
        /// this module.
        notification: Vec<u8>,
    },

    /// The remote has closed an incoming notifications substream.
    ///
    /// This can happen both before or after the notification substream has been accepted. If it
    /// happens before the substream has been accepted, this event should be interpreted as
    /// cancelling the opening.
    NotificationsInClose {
        /// Substream that has been closed. Guaranteed to match a substream that was earlier
        /// reported with a [`Event::NotificationsInOpen`].
        substream_id: SubstreamId,
        /// Reason why the substream has been closed.
        outcome: Result<(), NotificationsInClosedErr>, // TODO: other err for connection shutdown
    },

    /// An outgoing ping has succeeded. This event is generated automatically over time for each
    /// connection in the collection.
    PingOutSuccess { id: ConnectionId },
    /// An outgoing ping has failed. This event is generated automatically over time for each
    /// connection in the collection.
    PingOutFailed { id: ConnectionId },
}

/// Error within the context of a connection. See [`Network::read_write`].
#[derive(Debug, derive_more::Display)]
pub enum ConnectionError {
    /// Protocol error after the connection has been established.
    #[display(fmt = "{}", _0)]
    Established(established::Error),
    /// Eror during the handshake phase.
    #[display(fmt = "{}", _0)]
    Handshake(HandshakeError),
    /// Connection was shut down by calling [`Network::start_shutdown`].
    // TODO: that seems hacky
    LocalShutdown,
    /// Connection was gracefully terminated. Can only happen if the connection was established,
    /// as an EOF during the handshake is an error.
    Eof,
}

#[derive(Debug, derive_more::Display, Clone)]
pub enum RequestError {
    /// Request has been cancelled because the connection as a whole is being shut down.
    ConnectionShutdown,

    /// Error happened in the context of the substream.
    Substream(established::RequestError),
}

#[derive(Debug, derive_more::Display, Clone)]
pub enum NotificationsInClosedErr {
    /// Substream has been closed because the connection as a whole is being shut down.
    ConnectionShutdown,

    /// Error happened in the context of the substream.
    Substream(established::NotificationsInClosedErr),
}

/// Protocol error within the context of a connection. See [`Network::read_write`].
#[derive(Debug, derive_more::Display)]
pub enum HandshakeError {
    /// The handshake took too long.
    Timeout,
    /// Protocol error.
    #[display(fmt = "{}", _0)]
    Protocol(handshake::HandshakeError),
}

/// Error potentially returned by [`Network::queue_notification`].
#[derive(Debug, derive_more::Display)]
pub enum QueueNotificationError {
    /// Queue of notifications with that peer is full.
    QueueFull,
}
