// Smoldot
// Copyright (C) 2019-2022  Parity Technologies (UK) Ltd.
// SPDX-License-Identifier: GPL-3.0-or-later WITH Classpath-exception-2.0

// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.

// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.

// You should have received a copy of the GNU General Public License
// along with this program.  If not, see <http://www.gnu.org/licenses/>.

//! Background network service.
//!
//! The [`NetworkService`] manages background tasks dedicated to connecting to other nodes.
//! Importantly, its design is oriented towards the particular use case of the full node.
//!
//! The [`NetworkService`] spawns one background task (using the [`Config::tasks_executor`]) for
//! each active TCP socket, plus one for each TCP listening socket. Messages are exchanged between
//! the service and these background tasks.

// TODO: doc
// TODO: re-review this once finished

use crate::run::{database_thread, jaeger_service};

use core::{cmp, mem, task::Poll, time::Duration};
use futures::{
    channel::{mpsc, oneshot},
    lock::Mutex,
    prelude::*,
};
use futures_timer::Delay;
use hashbrown::HashMap;
use smoldot::{
    database::full_sqlite,
    header,
    informant::HashDisplay,
    libp2p::{
        async_std_connection::with_buffers,
        connection,
        multiaddr::{Multiaddr, ProtocolRef},
        peer_id::{self, PeerId},
        peers,
    },
    network::{protocol, service},
};
use std::{
    io, iter,
    net::{IpAddr, SocketAddr},
    num::NonZeroUsize,
    pin::Pin,
    sync::Arc,
    thread,
    time::Instant,
};
use tracing::Instrument as _;

/// Configuration for a [`NetworkService`].
pub struct Config<'a> {
    /// Closure that spawns background tasks.
    pub tasks_executor: &'a mut dyn FnMut(future::BoxFuture<'static, ()>),

    /// Number of event receivers returned by [`NetworkService::new`].
    pub num_events_receivers: usize,

    /// Addresses to listen for incoming connections.
    pub listen_addresses: Vec<Multiaddr>,

    /// List of block chains to be connected to.
    pub chains: Vec<ChainConfig>,

    /// Key used for the encryption layer.
    /// This is a Noise static key, according to the Noise specification.
    /// Signed using the actual libp2p key.
    pub noise_key: connection::NoiseKey,

    /// Service to use to report traces.
    pub jaeger_service: Arc<jaeger_service::JaegerService>,
}

/// Configuration for one chain.
pub struct ChainConfig {
    /// List of node identities and addresses that are known to belong to the chain's peer-to-pee
    /// network.
    pub bootstrap_nodes: Vec<(PeerId, Multiaddr)>,

    /// Database to use to read blocks from when answering requests.
    pub database: Arc<database_thread::DatabaseThread>,

    /// Hash of the genesis block of the chain. Sent to other nodes in order to determine whether
    /// the chains match.
    pub genesis_block_hash: [u8; 32],

    /// Number and hash of the current best block. Can later be updated with // TODO: which function?
    pub best_block: (u64, [u8; 32]),

    /// Identifier of the chain to connect to.
    ///
    /// Each blockchain has (or should have) a different "protocol id". This value identifies the
    /// chain, so as to not introduce conflicts in the networking messages.
    pub protocol_id: String,

    /// Number of bytes of the block number in the networking protocol.
    pub block_number_bytes: usize,

    /// If true, the chain uses the GrandPa networking protocol.
    pub has_grandpa_protocol: bool,
}

/// Event generated by the events reporters returned by [`NetworkService::new`].
#[derive(Debug, Clone)]
pub enum Event {
    Connected {
        chain_index: usize,
        peer_id: PeerId,
        best_block_number: u64,
        best_block_hash: [u8; 32],
    },
    Disconnected {
        chain_index: usize,
        peer_id: PeerId,
    },
    BlockAnnounce {
        chain_index: usize,
        peer_id: PeerId,
        header: header::Header,
        is_best: bool,
    },
}

pub struct NetworkService {
    /// Actual network service.
    inner: Arc<Inner>,

    /// Handles connected to all the background tasks of the network service. Makes it possible to
    /// abort everything.
    abort_handles: Vec<future::AbortHandle>,
}

struct Inner {
    /// Fields behind a mutex.
    guarded: Mutex<Guarded>,

    /// Event to notify when the background task needs to be waken up.
    ///
    /// Waking up this event guarantees a full loop of the background task. In other words,
    /// if the event is notified while the background task is already awake, the background task
    /// will do an additional loop.
    wake_up_main_background_task: event_listener::Event,

    /// Databases to use to read blocks from when answering requests.
    databases: Vec<Arc<database_thread::DatabaseThread>>,

    /// Identity of the local node.
    local_peer_id: PeerId,

    /// Service to use to report traces.
    jaeger_service: Arc<jaeger_service::JaegerService>,
}

struct Guarded {
    /// Data structure holding the entire state of the networking.
    network: service::ChainNetwork<Instant>,

    /// Current number of outgoing connection attempts.
    ///
    /// This counter is used to limit the number of simultaneous connection attempts, as some
    /// ISPs/cloud providers don't like seeing too many dialing connections at the same time.
    num_pending_out_attempts: usize,

    conn_tasks_tx: mpsc::Sender<Pin<Box<dyn Future<Output = ()> + Send>>>,

    active_connections: HashMap<
        service::ConnectionId,
        mpsc::Sender<service::CoordinatorToConnection<Instant>>,
        fnv::FnvBuildHasher,
    >,

    /// List of peer and chain index tuples for which no outbound slot should be assigned.
    ///
    /// The values are the moment when the ban expires.
    // TODO: use SipHasher
    slots_assign_backoff: HashMap<(PeerId, usize), Instant, fnv::FnvBuildHasher>,

    messages_from_connections_tx:
        mpsc::Sender<(service::ConnectionId, service::ConnectionToCoordinator)>,

    messages_from_connections_rx:
        mpsc::Receiver<(service::ConnectionId, service::ConnectionToCoordinator)>,

    /// List of all block requests that have been started but not finished yet.
    blocks_requests: HashMap<
        service::OutRequestId,
        oneshot::Sender<Result<Vec<protocol::BlockData>, service::BlocksRequestError>>,
        fnv::FnvBuildHasher,
    >,

    /// List of Kademlia discovery operations that have been started but not finished yet.
    kademlia_discovery_operations:
        HashMap<service::KademliaOperationId, usize, fnv::FnvBuildHasher>,
}

impl NetworkService {
    /// Initializes the network service with the given configuration.
    pub async fn new(
        config: Config<'_>,
    ) -> Result<(Arc<Self>, Vec<stream::BoxStream<'static, Event>>), InitError> {
        let (senders, receivers): (Vec<_>, Vec<_>) = (0..config.num_events_receivers)
            .map(|_| mpsc::channel(16))
            .unzip();

        let mut chains = Vec::with_capacity(config.chains.len());
        let mut databases = Vec::with_capacity(config.chains.len());
        for chain in &config.chains {
            chains.push(service::ChainConfig {
                in_slots: 25,
                out_slots: 25,
                protocol_id: chain.protocol_id.clone(),
                block_number_bytes: chain.block_number_bytes,
                best_hash: chain.best_block.1,
                best_number: chain.best_block.0,
                genesis_hash: chain.genesis_block_hash,
                role: protocol::Role::Full,
                grandpa_protocol_config: if chain.has_grandpa_protocol {
                    // TODO: dummy values
                    Some(service::GrandpaState {
                        commit_finalized_height: 0,
                        round_number: 1,
                        set_id: 0,
                    })
                } else {
                    None
                },
                allow_inbound_block_requests: true,
            });

            databases.push(chain.database.clone());
        }

        let mut network = service::ChainNetwork::new(service::Config {
            now: Instant::now(),
            chains,
            connections_capacity: 100, // TODO: ?
            peers_capacity: 100,       // TODO: ?
            noise_key: config.noise_key,
            handshake_timeout: Duration::from_secs(8),
            max_addresses_per_peer: NonZeroUsize::new(5).unwrap(),
            randomness_seed: rand::random(),
        });

        // Add the bootnodes to the inner state machine.
        for (chain_index, chain) in config.chains.into_iter().enumerate() {
            for (peer_id, addr) in chain.bootstrap_nodes {
                network.discover(&Instant::now(), chain_index, peer_id, iter::once(addr));
            }
        }

        // A channel is used to communicate new tasks dedicated to handling connections.
        let (conn_tasks_tx, mut conn_tasks_rx) = mpsc::channel(
            thread::available_parallelism()
                .map(|n| n.get())
                .unwrap_or(4),
        );

        // Initialize the inner network service.
        let inner = {
            let (messages_from_connections_tx, messages_from_connections_rx) = mpsc::channel(64);

            Arc::new(Inner {
                local_peer_id: peer_id::PublicKey::Ed25519(
                    *network.noise_key().libp2p_public_ed25519_key(),
                )
                .into_peer_id(),
                wake_up_main_background_task: event_listener::Event::new(),
                databases,
                guarded: Mutex::new(Guarded {
                    num_pending_out_attempts: 0,
                    messages_from_connections_tx,
                    messages_from_connections_rx,
                    conn_tasks_tx: conn_tasks_tx.clone(),
                    network,
                    slots_assign_backoff: hashbrown::HashMap::with_capacity_and_hasher(
                        50, // TODO: ?
                        Default::default(),
                    ),
                    active_connections: hashbrown::HashMap::with_capacity_and_hasher(
                        100, // TODO: ?
                        Default::default(),
                    ),
                    blocks_requests: hashbrown::HashMap::with_capacity_and_hasher(
                        50, // TODO: ?
                        Default::default(),
                    ),
                    kademlia_discovery_operations: hashbrown::HashMap::with_capacity_and_hasher(
                        4,
                        Default::default(),
                    ),
                }),
                jaeger_service: config.jaeger_service,
            })
        };

        let mut abort_handles = Vec::new();

        // Spawn the main task dedicated to processing the network.
        (config.tasks_executor)(Box::pin({
            let future = background_task(inner.clone(), senders);
            let (abortable, abort_handle) = future::abortable(
                future.instrument(tracing::trace_span!(parent: None, "network-background")),
            );
            abort_handles.push(abort_handle);
            abortable.map(|_| ())
        }));

        // Spawn tasks dedicated to starting the Kademlia discovery queries.
        for chain_index in 0..inner.databases.len() {
            (config.tasks_executor)(Box::pin({
                let inner = inner.clone();
                let future = async move {
                    let mut next_discovery = Duration::from_secs(1);

                    loop {
                        futures_timer::Delay::new(next_discovery).await;
                        next_discovery = cmp::min(next_discovery * 2, Duration::from_secs(120));

                        let mut guarded = inner.guarded.lock().await;
                        let operation_id = guarded
                            .network
                            .start_kademlia_discovery_round(Instant::now(), chain_index);
                        let _prev_val = guarded
                            .kademlia_discovery_operations
                            .insert(operation_id, chain_index);
                        debug_assert!(_prev_val.is_none());
                    }
                };

                let (abortable, abort_handle) = future::abortable(
                    future.instrument(tracing::trace_span!(parent: None, "kademlia-discovery")),
                );
                abort_handles.push(abort_handle);
                abortable.map(|_| ())
            }));
        }

        // For each listening address in the configuration, create a background task dedicated to
        // listening on that address.
        for listen_address in config.listen_addresses {
            // Try to parse the requested address and create the corresponding listening socket.
            let tcp_listener: async_std::net::TcpListener = {
                let addr = {
                    let mut iter = listen_address.iter();
                    let proto1 = iter.next();
                    let proto2 = iter.next();
                    let proto3 = iter.next();
                    match (proto1, proto2, proto3) {
                        (Some(ProtocolRef::Ip4(ip)), Some(ProtocolRef::Tcp(port)), None) => {
                            Some(SocketAddr::from((ip, port)))
                        }
                        (Some(ProtocolRef::Ip6(ip)), Some(ProtocolRef::Tcp(port)), None) => {
                            Some(SocketAddr::from((ip, port)))
                        }
                        _ => None,
                    }
                };

                if let Some(addr) = addr {
                    match async_std::net::TcpListener::bind(addr).await {
                        Ok(l) => l,
                        Err(err) => {
                            return Err(InitError::ListenerIo(listen_address, err));
                        }
                    }
                } else {
                    return Err(InitError::BadListenMultiaddr(listen_address));
                }
            };

            // Spawn a background task dedicated to this listener.
            (config.tasks_executor)(Box::pin({
                let mut conn_tasks_tx = conn_tasks_tx.clone();
                let inner = inner.clone();
                let future = async move {
                    loop {
                        let (socket, addr) = match tcp_listener.accept().await {
                            Ok(v) => v,
                            Err(_) => {
                                // Errors here can happen if the accept failed, for example if no
                                // file descriptor is available.
                                // A wait is added in order to avoid having a busy-loop failing to
                                // accept connections.
                                futures_timer::Delay::new(Duration::from_secs(2)).await;
                                continue;
                            }
                        };

                        // The Nagle algorithm, implemented in the kernel, consists in buffering the
                        // data to be sent out and waiting a bit before actually sending it out, in
                        // order to potentially merge multiple writes in a row into one packet. In
                        // the implementation below, it is guaranteed that the buffer in `WithBuffers`
                        // is filled with as much data as possible before the operating system gets
                        // involved. As such, we disable the Nagle algorithm, in order to avoid adding
                        // an artificial delay to all sends.
                        let _ = socket.set_nodelay(true);

                        let multiaddr = [
                            match addr.ip() {
                                IpAddr::V4(ip) => ProtocolRef::Ip4(ip.octets()),
                                IpAddr::V6(ip) => ProtocolRef::Ip6(ip.octets()),
                            },
                            ProtocolRef::Tcp(addr.port()),
                        ]
                        .into_iter()
                        .collect::<Multiaddr>();

                        tracing::debug!(%multiaddr, "incoming-connection");

                        let task = {
                            let mut guarded = inner.guarded.lock().await;
                            let (connection_id, connection_task) =
                                guarded.network.add_single_stream_incoming_connection(
                                    Instant::now(),
                                    multiaddr.clone(),
                                );

                            let (tx, rx) = mpsc::channel(16); // TODO: ?!
                            guarded.active_connections.insert(connection_id, tx);

                            established_connection_task(
                                socket,
                                inner.clone(),
                                connection_id,
                                connection_task,
                                rx,
                                guarded.messages_from_connections_tx.clone(),
                            )
                        };

                        // Ignore errors, as it is possible for the destination task to have been
                        // aborted already.
                        let _ = conn_tasks_tx.send(
                            task.instrument(
                                tracing::trace_span!(parent: None, "connection", address = %multiaddr),
                            ).boxed()
                        ).await;
                    }
                };

                let (abortable, abort_handle) = future::abortable(future.instrument(
                    tracing::trace_span!(parent: None, "listener", address = %listen_address),
                ));
                abort_handles.push(abort_handle);
                abortable.map(|_| ())
            }))
        }

        // Spawn task dedicated to processing connections.
        // A single task is responsible for all connections, thereby ensuring that the networking
        // won't use more than a single CPU core.
        (config.tasks_executor)(Box::pin({
            let future = async move {
                let mut connections = stream::FuturesUnordered::new();
                loop {
                    futures::select! {
                        new_connec = conn_tasks_rx.select_next_some() => {
                            connections.push(new_connec);
                        },
                        () = connections.select_next_some() => {},
                    }
                }
            };

            let (abortable, abort_handle) = future::abortable(
                future.instrument(tracing::trace_span!(parent: None, "connections-executor")),
            );
            abort_handles.push(abort_handle);
            abortable.map(|_| ())
        }));

        // Build the final network service.
        let network_service = Arc::new(NetworkService {
            inner,
            abort_handles: {
                abort_handles.shrink_to_fit();
                abort_handles
            },
        });

        // Adjust the receivers to keep the `network_service` alive.
        let receivers = receivers
            .into_iter()
            .map(|rx| {
                let mut network_service = Some(network_service.clone());
                rx.chain(stream::poll_fn(move |_| {
                    drop(network_service.take());
                    Poll::Ready(None)
                }))
                .boxed()
            })
            .collect();

        Ok((network_service, receivers))
    }

    /// Returns the number of established TCP connections, both incoming and outgoing.
    pub async fn num_established_connections(&self) -> usize {
        self.inner
            .guarded
            .lock()
            .await
            .network
            .num_established_connections()
    }

    /// Returns the number of peers we have a substream with.
    pub async fn num_peers(&self, chain_index: usize) -> usize {
        self.inner
            .guarded
            .lock()
            .await
            .network
            .num_peers(chain_index)
    }

    pub async fn set_local_best_block(
        &self,
        chain_index: usize,
        best_hash: [u8; 32],
        best_number: u64,
    ) {
        self.inner
            .guarded
            .lock()
            .await
            .network
            .set_local_best_block(chain_index, best_hash, best_number);
        self.inner.wake_up_main_background_task.notify(1);
    }

    pub async fn send_block_announce(
        self: Arc<Self>,
        target: &PeerId,
        chain_index: usize,
        scale_encoded_header: &[u8],
        is_best: bool,
    ) -> Result<(), QueueNotificationError> {
        let mut guarded = self.inner.guarded.lock().await;

        // The call to `send_block_announce` below panics if we have no active connection.
        // TODO: not the correct check; must make sure that we have a substream open
        if !guarded.network.has_established_connection(target) {
            return Err(QueueNotificationError::NoConnection);
        }

        let result = guarded
            .network
            .send_block_announce(target, chain_index, scale_encoded_header, is_best)
            .map_err(QueueNotificationError::Queue);

        self.inner.wake_up_main_background_task.notify(1);
        result
    }

    /// Sends a blocks request to the given peer.
    // TODO: more docs
    // TODO: proper error type
    #[tracing::instrument(level = "trace", skip(self))]
    pub async fn blocks_request(
        self: Arc<Self>,
        target: PeerId, // TODO: by value?
        chain_index: usize,
        config: protocol::BlocksRequestConfig,
    ) -> Result<Vec<protocol::BlockData>, BlocksRequestError> {
        tracing::debug!(
            peer_id = %target, %chain_index,
            start = %match &config.start {
                protocol::BlocksRequestConfigStart::Hash(h) => either::Left(HashDisplay(h)),
                protocol::BlocksRequestConfigStart::Number(n) => either::Right(n),
            },
            desired_count = config.desired_count,
            direction = match config.direction {
                protocol::BlocksRequestDirection::Ascending => "ascending",
                protocol::BlocksRequestDirection::Descending => "descending",
            },
            "blocks-request-start"
        );

        // Setup a guard that will print a log message in case it is dropped silently.
        // This lets us detect if the request is cancelled.
        struct LogIfCancel(PeerId, usize);
        impl Drop for LogIfCancel {
            fn drop(&mut self) {
                tracing::debug!(
                    peer_id = %self.0,
                    chain_index = %self.1,
                    outcome = "cancelled",
                    "blocks-request-ended"
                );
            }
        }
        let _log_if_cancel = LogIfCancel(target.clone(), chain_index);

        let _jaeger_span = self.inner.jaeger_service.outgoing_block_request_span(
            &self.inner.local_peer_id,
            &target,
            config.desired_count.get(),
            if let (1, protocol::BlocksRequestConfigStart::Hash(block_hash)) =
                (config.desired_count.get(), &config.start)
            {
                Some(block_hash)
            } else {
                None
            },
        );

        let rx = {
            let mut guarded = self.inner.guarded.lock().await;

            // The call to `start_blocks_request` below panics if we have no active connection.
            if !guarded.network.has_established_connection(&target) {
                return Err(BlocksRequestError::NoConnection);
            }

            let (tx, rx) = oneshot::channel();

            let request_id = guarded.network.start_blocks_request(
                Instant::now(),
                &target,
                chain_index,
                config,
                Duration::from_secs(12),
            );

            // TODO: somehow cancel the request if the `rx` is dropped?
            guarded.blocks_requests.insert(request_id, tx);

            self.inner.wake_up_main_background_task.notify(1);
            rx
        };

        let result = rx.await.unwrap().map_err(BlocksRequestError::Request);

        // Requet has finished. Print the log and prevent the cancellation message from being
        // printed.
        mem::forget(_log_if_cancel);
        match &result {
            Ok(success) => {
                tracing::debug!(
                    peer_id = %target, %chain_index,
                    outcome = "success",
                    response_blocks = success.len(),
                    "blocks-request-ended"
                );
            }
            Err(err) => {
                tracing::debug!(
                    peer_id = %target, %chain_index,
                    outcome = "failure",
                    error = %err,
                    "blocks-request-ended"
                );
            }
        }

        result
    }
}

impl Drop for NetworkService {
    fn drop(&mut self) {
        for handle in &mut self.abort_handles {
            handle.abort();
        }
    }
}

/// Error when initializing the network service.
#[derive(Debug, derive_more::Display)]
pub enum InitError {
    /// I/O error when initializing a listener.
    #[display(fmt = "I/O error when creating listener for {}: {}", _0, _1)]
    ListenerIo(Multiaddr, io::Error),
    /// A listening address passed through the configuration isn't valid.
    #[display(
        fmt = "A listening address passed through the configuration isn't valid: {}",
        _0
    )]
    BadListenMultiaddr(Multiaddr),
}

/// Error returned by [`NetworkService::blocks_request`].
#[derive(Debug, derive_more::Display)]
pub enum BlocksRequestError {
    /// No established connection with the target.
    NoConnection,
    /// Error during the request.
    #[display(fmt = "{}", _0)]
    Request(service::BlocksRequestError),
}

/// Error returned by [`NetworkService::send_block_announce`].
#[derive(Debug, derive_more::Display)]
pub enum QueueNotificationError {
    /// No established connection with the target.
    NoConnection,
    /// Error during the queuing.
    #[display(fmt = "{}", _0)]
    Queue(peers::QueueNotificationError),
}

async fn background_task(inner: Arc<Inner>, mut event_senders: Vec<mpsc::Sender<Event>>) {
    loop {
        // In order to guarantee that waking up `wake_up_background` will run an entirely
        // loop of `update_round`, we grab the listener at the start. If `wake_up_background`
        // is notified while `update_round` is running, the `notified.await` below will be
        // instantaneous.
        let notified = inner.wake_up_main_background_task.listen();
        update_round(&inner, &mut event_senders).await;
        notified.await;
    }
}

async fn update_round(inner: &Arc<Inner>, event_senders: &mut [mpsc::Sender<Event>]) {
    let mut guarded = inner.guarded.lock().await;

    // Inject in the coordinator the messages that the connections have generated.
    loop {
        let (connection_id, message) =
            match guarded.messages_from_connections_rx.next().now_or_never() {
                Some(Some(v)) => v,
                _ => break,
            };

        guarded
            .network
            .inject_connection_message(connection_id, message);
    }

    // Process events generated by the state machine.
    'events_loop: loop {
        let event = loop {
            let inner_event = match guarded.network.next_event(Instant::now()) {
                Some(ev) => ev,
                None => break 'events_loop,
            };

            match inner_event {
                service::Event::Connected(peer_id) => {
                    tracing::debug!(%peer_id, "connected");
                }
                service::Event::Disconnected {
                    peer_id,
                    chain_indices,
                } => {
                    tracing::debug!(%peer_id, "disconnected");
                    if !chain_indices.is_empty() {
                        debug_assert_eq!(chain_indices.len(), 1); // TODO: not implemented
                        break Event::Disconnected {
                            chain_index: chain_indices[0],
                            peer_id,
                        };
                    }
                }
                service::Event::BlockAnnounce {
                    chain_index,
                    peer_id,
                    announce,
                } => {
                    let decoded = announce.decode();
                    let header_hash =
                        header::hash_from_scale_encoded_header(&decoded.scale_encoded_header);
                    match header::decode(
                        decoded.scale_encoded_header,
                        guarded.network.block_number_bytes(chain_index),
                    ) {
                        Ok(decoded_header) => {
                            let mut _jaeger_span =
                                inner.jaeger_service.block_announce_receive_span(
                                    &inner.local_peer_id,
                                    &peer_id,
                                    decoded_header.number,
                                    &decoded_header
                                        .hash(guarded.network.block_number_bytes(chain_index)),
                                );

                            tracing::debug!(
                                %chain_index, %peer_id,
                                hash = %HashDisplay(&header_hash),
                                number = decoded_header.number,
                                is_best = ?decoded.is_best,
                                "block-announce"
                            );

                            break Event::BlockAnnounce {
                                chain_index,
                                peer_id,
                                is_best: decoded.is_best,
                                header: decoded_header.into(), // TODO: somewhat wasteful allocation here
                            };
                        }
                        Err(error) => {
                            tracing::warn!(
                                %chain_index, %peer_id,
                                hash = %HashDisplay(&header_hash),
                                is_best = ?decoded.is_best,
                                %error,
                                "block-announce-bad-header"
                            );

                            guarded.unassign_slot_and_ban(chain_index, peer_id);
                            inner.wake_up_main_background_task.notify(1);
                        }
                    }
                }
                service::Event::ChainConnected {
                    peer_id,
                    chain_index,
                    best_number,
                    best_hash,
                    ..
                } => {
                    tracing::debug!(
                        %peer_id,
                        %best_number,
                        best_hash = %HashDisplay(&best_hash),
                        "chain-connected"
                    );
                    break Event::Connected {
                        peer_id,
                        chain_index,
                        best_block_number: best_number,
                        best_block_hash: best_hash,
                    };
                }
                service::Event::ChainDisconnected {
                    peer_id,
                    chain_index,
                    ..
                } => {
                    tracing::debug!(%peer_id, "chain-disconnected");

                    guarded.unassign_slot_and_ban(chain_index, peer_id.clone());
                    inner.wake_up_main_background_task.notify(1);

                    break Event::Disconnected {
                        chain_index,
                        peer_id,
                    };
                }
                service::Event::ChainConnectAttemptFailed {
                    chain_index,
                    peer_id,
                    error,
                    ..
                } => {
                    tracing::debug!(
                        %peer_id, %error,
                        "chain-connect-attempt-failed"
                    );

                    guarded.unassign_slot_and_ban(chain_index, peer_id);
                    inner.wake_up_main_background_task.notify(1);
                }
                service::Event::InboundSlotAssigned { .. } => {
                    // TODO: log this
                }
                service::Event::BlocksRequestResult {
                    request_id,
                    response,
                } => {
                    let _ = guarded
                        .blocks_requests
                        .remove(&request_id)
                        .unwrap()
                        .send(response);
                }
                service::Event::GrandpaWarpSyncRequestResult { .. }
                | service::Event::StateRequestResult { .. }
                | service::Event::StorageProofRequestResult { .. }
                | service::Event::CallProofRequestResult { .. }
                | service::Event::KademliaFindNodeRequestResult { .. } => {
                    // We never start a request of this kind.
                    unreachable!()
                }
                service::Event::RequestInCancel { .. } => {
                    // Requests are answered immediately, and thus cancelling events can't happen.
                    unreachable!()
                }
                service::Event::KademliaDiscoveryResult {
                    operation_id,
                    result,
                } => {
                    let chain_index = guarded
                        .kademlia_discovery_operations
                        .remove(&operation_id)
                        .unwrap();
                    match result {
                        Ok(nodes) => {
                            tracing::debug!(discovered = ?nodes, "discovered");
                            for (peer_id, addrs) in nodes {
                                guarded.network.discover(
                                    &Instant::now(),
                                    chain_index,
                                    peer_id,
                                    addrs,
                                );
                            }
                        }
                        Err(error) => {
                            tracing::debug!(%error, "discovery-error");
                        }
                    }
                }
                service::Event::IdentifyRequestIn {
                    peer_id,
                    request_id,
                } => {
                    tracing::debug!(%peer_id, "identify-request");
                    guarded.network.respond_identify(request_id, "smoldot");
                }
                service::Event::BlocksRequestIn {
                    peer_id,
                    chain_index,
                    config,
                    request_id,
                } => {
                    tracing::debug!(%peer_id, "incoming-blocks-request");
                    let mut _jaeger_span = inner.jaeger_service.incoming_block_request_span(
                        &inner.local_peer_id,
                        &peer_id,
                        config.desired_count.get(),
                        if let (1, protocol::BlocksRequestConfigStart::Hash(block_hash)) =
                            (config.desired_count.get(), &config.start)
                        {
                            Some(block_hash)
                        } else {
                            None
                        },
                    );

                    // TODO: is it a good idea to await here while the lock is held and freezing the entire networking background task?
                    let response = blocks_request_response(
                        &inner.databases[chain_index],
                        guarded.network.block_number_bytes(chain_index),
                        config,
                    )
                    .await;
                    guarded.network.respond_blocks(
                        request_id,
                        match response {
                            Ok(b) => Some(b),
                            Err(error) => {
                                tracing::warn!(%error, "incoming-blocks-request-error");
                                None
                            }
                        },
                    );
                }
                service::Event::GrandpaCommitMessage {
                    chain_index,
                    peer_id,
                    message,
                } => {
                    tracing::debug!(
                        %chain_index, %peer_id,
                        target_hash = %HashDisplay(message.decode().message.target_hash),
                        "grandpa-commit-message"
                    );
                }
                service::Event::ProtocolError { peer_id, error } => {
                    tracing::warn!(
                        %peer_id,
                        %error,
                        "protocol-error"
                    );

                    for chain_index in 0..guarded.network.num_chains() {
                        guarded.unassign_slot_and_ban(chain_index, peer_id.clone());
                    }
                    inner.wake_up_main_background_task.notify(1);
                }
            }
        };

        // Dispatch the event to the various senders.

        // Because the tasks processing the receivers might be waiting to acquire the lock, we
        // need to unlock the lock before sending. This guarantees that the sending finishes at
        // some point in the future.
        drop(guarded);

        // This little `if` avoids having to do `event.clone()` if we don't have to.
        if event_senders.len() == 1 {
            let _ = event_senders[0].send(event).await;
        } else {
            for sender in event_senders.iter_mut() {
                // For simplicity we don't get rid of closed senders because senders aren't
                // supposed to close, and that leaving closed senders in the list doesn't have any
                // consequence other than one extra iteration every time.
                let _ = sender.send(event.clone()).await;
            }
        }

        // Re-acquire lock to continue the function.
        guarded = inner.guarded.lock().await;
    }

    // TODO: doc
    for chain_index in 0..guarded.network.num_chains() {
        let now = Instant::now();

        // Clean up the content of `slots_assign_backoff`.
        // TODO: the background task should be woken up when the ban expires
        // TODO: O(n)
        guarded
            .slots_assign_backoff
            .retain(|_, expiration| *expiration > now);

        // Assign outgoing slots.
        loop {
            let peer_to_assign = guarded
                .network
                .slots_to_assign(chain_index)
                .filter(|peer_id| {
                    !guarded
                        .slots_assign_backoff
                        .contains_key(&((**peer_id).clone(), chain_index)) // TODO: spurious cloning
                })
                .next()
                .cloned();

            if let Some(peer_to_assign) = peer_to_assign {
                tracing::debug!(peer_id = %peer_to_assign, %chain_index, "slot-assigned");
                guarded.network.assign_out_slot(chain_index, peer_to_assign);
            } else {
                break;
            }
        }
    }

    // The networking service contains a list of connections that should be opened.
    // Grab this list and start opening a connection for each.
    // TODO: restore the rate limiting for connections openings
    loop {
        if guarded.num_pending_out_attempts >= 16 {
            // TODO: constant
            break;
        }

        let start_connect = match guarded.network.next_start_connect(Instant::now) {
            Some(sc) => sc,
            None => break,
        };

        guarded.num_pending_out_attempts += 1;

        // Perform the connection process in a separate task.
        let task = opening_connection_task(
            start_connect,
            inner.clone(),
            guarded.messages_from_connections_tx.clone(),
        );

        // Sending the new task might fail in case a shutdown is happening, in which case
        // we don't really care about the state of anything anymore.
        // The sending here is normally very quick.
        let _ = guarded.conn_tasks_tx.send(Box::pin(task)).await;
    }

    // Pull messages that the coordinator has generated in destination to the various
    // connections.
    while let Some((connection_id, message)) = guarded.network.pull_message_to_connection() {
        // Note that it is critical for the sending to not take too long here, in order to not
        // block the process of the network service.
        // In particular, if sending the message to the connection is blocked due to sending
        // a message on the connection-to-coordinator channel, this will result in a deadlock.
        // For this reason, the connection task is always ready to immediately accept a message
        // on the coordinator-to-connection channel.
        guarded
            .active_connections
            .get_mut(&connection_id)
            .unwrap()
            .send(message)
            .await
            .unwrap();
    }
}

impl Guarded {
    fn unassign_slot_and_ban(&mut self, chain_index: usize, peer_id: PeerId) {
        self.network.unassign_slot(chain_index, &peer_id);

        let new_expiration = Instant::now() + Duration::from_secs(20); // TODO: arbitrary constant
        match self.slots_assign_backoff.entry((peer_id, chain_index)) {
            hashbrown::hash_map::Entry::Occupied(e) if *e.get() < new_expiration => {
                *e.into_mut() = new_expiration;
            }
            hashbrown::hash_map::Entry::Occupied(_) => {}
            hashbrown::hash_map::Entry::Vacant(e) => {
                e.insert(new_expiration);
            }
        }
    }
}

/// Asynchronous task managing a specific connection, including the dialing process.
#[tracing::instrument(level = "trace", skip(start_connect, inner, connection_to_coordinator))]
async fn opening_connection_task(
    start_connect: service::StartConnect<Instant>,
    inner: Arc<Inner>,
    connection_to_coordinator: mpsc::Sender<(
        service::ConnectionId,
        service::ConnectionToCoordinator,
    )>,
) {
    let span = tracing::debug_span!("start-connect", ?start_connect.id, %start_connect.multiaddr);
    let _enter = span.enter();

    // Convert the `multiaddr` (typically of the form `/ip4/a.b.c.d/tcp/d`) into
    // a `Future<dyn Output = Result<TcpStream, ...>>`.
    let socket = match multiaddr_to_socket(&start_connect.multiaddr) {
        Ok(socket) => socket,
        Err(_) => {
            tracing::debug!(%start_connect.multiaddr, "not-tcp");
            drop(_enter);

            let mut guarded = inner.guarded.lock().await;
            guarded.num_pending_out_attempts -= 1;
            guarded.network.pending_outcome_err(start_connect.id, true);
            inner.wake_up_main_background_task.notify(1);
            return;
        }
    };
    drop(_enter);

    // Finishing ongoing connection process.
    let socket = {
        let now = Instant::now();
        let mut timeout = Delay::new(if start_connect.timeout >= now {
            start_connect.timeout - now
        } else {
            // `timeout - now` would panic
            Duration::new(0, 0)
        })
        .fuse();
        let socket = socket.fuse();
        futures::pin_mut!(socket);
        futures::select! {
            _ = timeout => {
                let mut guarded = inner.guarded.lock().await;
                guarded.num_pending_out_attempts -= 1;
                guarded.network.pending_outcome_err(start_connect.id, false);
                inner.wake_up_main_background_task.notify(1);
                return;
            }
            result = socket => {
                match result {
                    Ok(s) => s,
                    Err(_) => {
                        let mut guarded = inner.guarded.lock().await;
                        guarded.num_pending_out_attempts -= 1;
                        guarded.network.pending_outcome_err(start_connect.id, true);
                        inner.wake_up_main_background_task.notify(1);
                        return;
                    }
                }
            }
        }
    };

    // Inform the underlying network state machine that this dialing attempt
    // has succeeded.
    let mut guarded = inner.guarded.lock().await;
    guarded.num_pending_out_attempts -= 1;
    let (connection_id, connection_task) = guarded
        .network
        .pending_outcome_ok_single_stream(start_connect.id);
    inner.wake_up_main_background_task.notify(1);

    let (tx, rx) = mpsc::channel(16); // TODO: ?!
    guarded.active_connections.insert(connection_id, tx);

    // Now run the connection.
    drop(guarded);
    established_connection_task(
        socket,
        inner,
        connection_id,
        connection_task,
        rx,
        connection_to_coordinator,
    )
    .await;
}

/// Asynchronous task managing a specific connection.
#[tracing::instrument(
    level = "trace",
    skip(
        tcp_socket,
        inner,
        connection_task,
        coordinator_to_connection,
        connection_to_coordinator
    )
)]
async fn established_connection_task(
    tcp_socket: async_std::net::TcpStream,
    inner: Arc<Inner>,
    connection_id: service::ConnectionId,
    mut connection_task: service::SingleStreamConnectionTask<Instant>,
    coordinator_to_connection: mpsc::Receiver<service::CoordinatorToConnection<Instant>>,
    mut connection_to_coordinator: mpsc::Sender<(
        service::ConnectionId,
        service::ConnectionToCoordinator,
    )>,
) {
    // The socket is wrapped around a `WithBuffers` object containing a read buffer and a write
    // buffer. These are the buffers whose pointer is passed to `read(2)` and `write(2)` when
    // reading/writing the socket.
    //
    // Contains `None` if an I/O error has happened on the socket in the past.
    let mut tcp_socket = Some(with_buffers::WithBuffers::new(tcp_socket));

    // We need to use `peek()` on this future later down this function.
    let mut coordinator_to_connection = coordinator_to_connection.peekable();

    loop {
        // Inject in the connection task the messages coming from the coordinator, if any.
        loop {
            let message = match coordinator_to_connection.next().now_or_never() {
                Some(Some(msg)) => msg,
                _ => break,
            };
            connection_task.inject_coordinator_message(message);
        }

        let wake_up_after = if let Some(socket) = tcp_socket.as_mut() {
            let (read_buffer, write_buffer) = match socket.buffers() {
                Ok(b) => b,
                Err(error) => {
                    tracing::debug!(%error, "connection-error");
                    connection_task.reset();
                    tcp_socket = None;
                    continue;
                }
            };

            let outgoing_buffer_was_closed = write_buffer.is_none();

            let mut read_write = service::ReadWrite {
                now: Instant::now(),
                incoming_buffer: read_buffer.map(|b| b.0),
                outgoing_buffer: write_buffer,
                read_bytes: 0,
                written_bytes: 0,
                wake_up_after: None,
            };

            connection_task.read_write(&mut read_write);

            if read_write.read_bytes != 0
                || read_write.written_bytes != 0
                || read_write.outgoing_buffer.is_none()
            {
                tracing::event!(
                    tracing::Level::TRACE,
                    read = read_write.read_bytes,
                    written = read_write.written_bytes,
                    "wake-up" = ?read_write.wake_up_after,  // TODO: ugly display
                    "write-close" = read_write.outgoing_buffer.is_none(),
                );
            }

            // We need to destroy `read_write` in order to un-borrow `socket`.
            let read_bytes = read_write.read_bytes;
            let written_bytes = read_write.written_bytes;
            let wake_up_after = read_write.wake_up_after.take();
            let outgoing_buffer_now_closed = read_write.outgoing_buffer.is_none();

            if outgoing_buffer_now_closed && !outgoing_buffer_was_closed {
                socket.close();
            }

            socket.advance(read_bytes, written_bytes);

            if read_bytes != 0 || written_bytes != 0 {
                continue;
            }

            wake_up_after
        } else {
            None
        };

        // Try pull message to send to the coordinator.

        // Calling this method takes ownership of the task and returns that task if it has
        // more work to do. If `None` is returned, then the entire task is gone and the
        // connection must be abruptly closed, which is what happens when we return from
        // this function.
        let (mut task_update, message) = connection_task.pull_message_to_coordinator();

        // If `task_update` is `None`, the connection task is going to die as soon as the
        // message reaches the coordinator. Before returning, we need to do a bit of clean up
        // by removing the task from the list of active connections.
        // This is done before the message is sent to the coordinator, in order to be sure
        // that the connection id is still attributed to the current task, and not to a new
        // connection that the coordinator has assigned after receiving the message.
        if task_update.is_none() {
            let mut guarded = inner.guarded.lock().await;
            let _was_in = guarded.active_connections.remove(&connection_id);
            debug_assert!(_was_in.is_some());
        }

        if let Some(message) = message {
            // Sending this message might take a long time (in case the coordinator is busy),
            // but this is intentional and serves as a back-pressure mechanism.
            // However, it is important to continue processing the messages coming from the
            // coordinator, otherwise this could result in a deadlock.

            // We do this by waiting for `connection_to_coordinator` to be ready to accept
            // an element. Due to the way channels work, once a channel is ready it will
            // always remain ready until we push an element. While waiting, we process
            // incoming messages.
            loop {
                futures::select! {
                    _ = future::poll_fn(|cx| connection_to_coordinator.poll_ready(cx)).fuse() => break,
                    message = coordinator_to_connection.next() => {
                        if let Some(message) = message {
                            if let Some(task_update) = &mut task_update {
                                task_update.inject_coordinator_message(message);
                            }
                        } else {
                            return;
                        }
                    }
                }
            }
            let result = connection_to_coordinator.try_send((connection_id, message));
            inner.wake_up_main_background_task.notify(1);
            if result.is_err() {
                return;
            }
        }

        if let Some(task_update) = task_update {
            connection_task = task_update;
        } else {
            return;
        }

        // Starting from here, we block the current task until more processing needs to happen.

        // Future ready when the timeout indicated by the connection state machine is reached.
        let poll_after = if let Some(wake_up) = wake_up_after {
            let now = Instant::now();
            if wake_up > now {
                let dur = wake_up - now;
                future::Either::Left(futures_timer::Delay::new(dur))
            } else {
                // "Wake up" immediately.
                continue;
            }
        } else {
            future::Either::Right(future::pending())
        }
        .fuse();

        // Future that is woken up when new data is ready on the socket.
        let connection_ready = if let Some(tcp_socket) = tcp_socket.as_mut() {
            future::Either::Left(Pin::new(tcp_socket).process())
        } else {
            future::Either::Right(future::pending())
        };

        // Future that is woken up when a new message is coming from the coordinator.
        let message_from_coordinator = Pin::new(&mut coordinator_to_connection).peek();

        // Wait until either some data is ready on the socket, or the connection state machine
        // has requested to be polled again, or a message is coming from the coordinator.
        futures::pin_mut!(connection_ready);
        future::select(
            future::select(connection_ready, message_from_coordinator),
            poll_after,
        )
        .await;
    }
}

/// Builds a future that connects to the given multiaddress. Returns an error if the multiaddress
/// protocols aren't supported.
fn multiaddr_to_socket(
    addr: &Multiaddr,
) -> Result<impl Future<Output = Result<async_std::net::TcpStream, io::Error>>, ()> {
    let mut iter = addr.iter();
    let proto1 = iter.next().ok_or(())?;
    let proto2 = iter.next().ok_or(())?;

    if iter.next().is_some() {
        return Err(());
    }

    // Ensure ahead of time that the multiaddress is supported.
    let addr = match (&proto1, &proto2) {
        (ProtocolRef::Ip4(ip), ProtocolRef::Tcp(port)) => {
            either::Left(SocketAddr::new(IpAddr::V4((*ip).into()), *port))
        }
        (ProtocolRef::Ip6(ip), ProtocolRef::Tcp(port)) => {
            either::Left(SocketAddr::new(IpAddr::V6((*ip).into()), *port))
        }
        // TODO: we don't care about the differences between Dns, Dns4, and Dns6
        (ProtocolRef::Dns(addr), ProtocolRef::Tcp(port)) => {
            either::Right((addr.to_string(), *port))
        }
        (ProtocolRef::Dns4(addr), ProtocolRef::Tcp(port)) => {
            either::Right((addr.to_string(), *port))
        }
        (ProtocolRef::Dns6(addr), ProtocolRef::Tcp(port)) => {
            either::Right((addr.to_string(), *port))
        }
        _ => return Err(()),
    };

    Ok(async move {
        match addr {
            either::Left(socket_addr) => {
                let tcp_socket = async_std::net::TcpStream::connect(socket_addr).await;

                if let Ok(tcp_socket) = &tcp_socket {
                    // The Nagle algorithm, implemented in the kernel, consists in buffering the
                    // data to be sent out and waiting a bit before actually sending it out, in
                    // order to potentially merge multiple writes in a row into one packet. In
                    // the implementation below, it is guaranteed that the buffer in `WithBuffers`
                    // is filled with as much data as possible before the operating system gets
                    // involved. As such, we disable the Nagle algorithm, in order to avoid adding
                    // an artificial delay to all sends.
                    let _ = tcp_socket.set_nodelay(true);
                }

                tcp_socket
            }
            either::Right((dns, port)) => {
                async_std::net::TcpStream::connect((&dns[..], port)).await
            }
        }
    })
}

/// Builds the response to a block request by reading from the given database.
async fn blocks_request_response(
    database: &database_thread::DatabaseThread,
    block_number_bytes: usize,
    config: protocol::BlocksRequestConfig,
) -> Result<Vec<protocol::BlockData>, full_sqlite::AccessError> {
    database
        .with_database(move |database| {
            let num_blocks = cmp::min(
                usize::try_from(config.desired_count.get()).unwrap_or(usize::max_value()),
                128,
            );

            let mut output = Vec::with_capacity(num_blocks);
            let mut next_block = config.start;

            loop {
                if output.len() >= num_blocks {
                    break;
                }

                let hash = match next_block {
                    protocol::BlocksRequestConfigStart::Hash(hash) => hash,
                    protocol::BlocksRequestConfigStart::Number(number) => {
                        // TODO: naive block selection ; should choose the best chain instead
                        match database.block_hash_by_number(number)?.next() {
                            Some(h) => h,
                            None => break,
                        }
                    }
                };

                let header = match database.block_scale_encoded_header(&hash)? {
                    Some(h) => h,
                    None => break,
                };

                next_block = {
                    let decoded = header::decode(&header, block_number_bytes).unwrap();
                    match config.direction {
                        protocol::BlocksRequestDirection::Ascending => {
                            protocol::BlocksRequestConfigStart::Hash(*decoded.parent_hash)
                        }
                        protocol::BlocksRequestDirection::Descending => {
                            // TODO: right now, since we don't necessarily pick the best chain in `block_hash_by_number`, it is possible that the next block doesn't have the current block as parent
                            protocol::BlocksRequestConfigStart::Number(decoded.number + 1)
                        }
                    }
                };

                output.push(protocol::BlockData {
                    hash,
                    header: if config.fields.header {
                        Some(header)
                    } else {
                        None
                    },
                    body: if config.fields.body {
                        Some(match database.block_extrinsics(&hash)? {
                            Some(body) => body.collect(),
                            None => break,
                        })
                    } else {
                        None
                    },
                    justifications: if config.fields.justifications {
                        // TODO: justifications aren't saved in database at the moment
                        Some(Vec::new())
                    } else {
                        None
                    },
                });
            }

            Ok(output)
        })
        .await
}
